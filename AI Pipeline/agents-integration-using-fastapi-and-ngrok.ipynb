{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install crewai crewai-tools \\\n",
    "#     langchain langchain-community langchain-google-genai \\\n",
    "#     langchain-huggingface sentence-transformers chromadb \\\n",
    "#     google-generativeai duckduckgo-search reportlab \\\n",
    "#     pydantic \n",
    "# !pip install pymupdf pytesseract pillow\n",
    "# !apt-get update\n",
    "# !apt-get install -y tesseract-ocr tesseract-ocr-ara\n",
    "# !pip install arabic_reshaper\n",
    "# !pip install neo4j pyngrok\n",
    "# !pip install langchain_experimental\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Helpers**\n",
    "This Cell performs OCR on Arabic PDFs using PyMuPDF, Tesseract, and Pillow.\n",
    "It preprocesses and caches OCR data, manages Arabic text rendering with ReportLab and supports RTL formatting.\n",
    "Additionally, it interfaces with Neo4j for knowledge graph operations and generates PDF reports with images and text styling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T13:36:12.921584Z",
     "iopub.status.busy": "2025-06-12T13:36:12.921315Z",
     "iopub.status.idle": "2025-06-12T13:36:14.300079Z",
     "shell.execute_reply": "2025-06-12T13:36:14.299344Z",
     "shell.execute_reply.started": "2025-06-12T13:36:12.921560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from typing import Any, List, Tuple, Type  # Added Type import\n",
    "# ╭──────────────── ReportLab + RTL helpers ────────────╮\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas as pdf_canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "ARABIC_FONT_PATH = \"/kaggle/input/nottoooo/NotoNaskhArabic-Regular.ttf\"     # ← change if needed\n",
    "ARABIC_FONT_NAME = \"NotoArabic\"\n",
    "ARABIC_FONT_PATH_bold = \"/kaggle/input/text-bold/NotoNaskhArabic-Bold.ttf\"     # ← change if needed\n",
    "ARABIC_FONT_NAME_bold = \"NotoArabic-Bold\"\n",
    "pdfmetrics.registerFont(TTFont(ARABIC_FONT_NAME, ARABIC_FONT_PATH))\n",
    "pdfmetrics.registerFont(TTFont(ARABIC_FONT_NAME_bold, ARABIC_FONT_PATH_bold))\n",
    "\n",
    "import re\n",
    "from PIL import Image   # Pillow is installed in Kaggle images\n",
    "\n",
    "IMG_DIR = \"/kaggle/input/book-images\"     # adjust if your folder differs\n",
    "MAX_IMG_W = 180                          # pixel width allowed on page\n",
    "MAX_IMG_H = 140                          # pixel height allowed on page\n",
    "\n",
    "MD_IMG = re.compile(r'!\\[(.*?)\\]\\((.*?)\\)')   # ![alt](path)\n",
    "\n",
    "\n",
    "def wrap_arabic(text: str, max_chars: int = 70) -> List[str]:\n",
    "    \"\"\"Naive word-wrap for Arabic lines.\"\"\"\n",
    "    words = text.split()\n",
    "    lines, buf = [], []\n",
    "    for w in words:\n",
    "        if sum(len(x) for x in buf) + len(w) + len(buf) > max_chars:\n",
    "            lines.append(\" \".join(buf))\n",
    "            buf = [w]\n",
    "        else:\n",
    "            buf.append(w)\n",
    "    if buf:\n",
    "        lines.append(\" \".join(buf))\n",
    "    return lines\n",
    "\n",
    "def strip_unsupported(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove any character that is not:\n",
    "      - Arabic letters (U+0600–U+06FF)\n",
    "      - Basic Latin letters/digits/punctuation (U+0000–U+007F)\n",
    "      - Common Arabic punctuation: ، ؟ ! - (and space)\n",
    "    This effectively strips emojis and other symbols that the Arabic font cannot render.\n",
    "    \"\"\"\n",
    "    # Allow U+0600..U+06FF (Arabic), U+0000..U+007F (Basic Latin),\n",
    "    # and the Arabic comma (U+060C) and question mark (U+061F) and exclamation (U+0021) and dash/hyphen.\n",
    "    return re.sub(r\"[^\\u0000-\\u007F\\u0600-\\u06FF\\u060C\\u061F\\u0021\\u002D\\s]\", \"\", text)\n",
    "\n",
    "# Set Tesseract language data path\n",
    "os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata/'\n",
    "class SessionMemory(dict):\n",
    "    def log(self, key: str, value: Any):\n",
    "        self.setdefault(key, []).append(value)\n",
    "\n",
    "def load_arabic_pdf(pdf_path, lang=\"ara\", batch_size=40, cache_dir=\"/kaggle/working/\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    pdf_name = os.path.basename(pdf_path)\n",
    "    cache_file = os.path.join(cache_dir, pdf_name + \".json\")\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached OCR data from {cache_file}\")\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_docs = json.load(f)\n",
    "        return [Document(page_content=d[\"page_content\"], metadata=d[\"metadata\"]) for d in raw_docs]\n",
    "\n",
    "    documents = []\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "\n",
    "        for start in range(0, total_pages, batch_size):\n",
    "            end = min(start + batch_size, total_pages)\n",
    "            print(f\"Processing pages {start+1} to {end}...\")\n",
    "\n",
    "            for i in range(start, end):\n",
    "                page = doc[i]\n",
    "                pix = page.get_pixmap(dpi=300)\n",
    "                img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "                text = pytesseract.image_to_string(img, lang=lang)\n",
    "\n",
    "                documents.append(\n",
    "                    Document(\n",
    "                        page_content=text.strip(),\n",
    "                        metadata={\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": i,\n",
    "                            \"total_pages\": total_pages,\n",
    "                            \"page_label\": str(i + 1)\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(\n",
    "                [{\"page_content\": d.page_content, \"metadata\": d.metadata} for d in documents],\n",
    "                f,\n",
    "                ensure_ascii=False,\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jKG:\n",
    "    def __init__(self, uri: str, user: str, pwd: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def get_lessons_for_topic(self, topic_name: str) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH (t:Topic {name: $topic_name})-[:HAS_LESSON]->(l:Lesson)\n",
    "        RETURN l.title AS title, l.start_page AS start_page, l.end_page AS end_page\n",
    "        ORDER BY l.title\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, topic_name=topic_name)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    def find_branch_for_topic(self, topic_name: str) -> Optional[str]:\n",
    "        query = \"\"\"\n",
    "        MATCH (b:Branch)-[:HAS_TOPIC]->(t:Topic {name: $topic_name})\n",
    "        RETURN b.name AS branch_name\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            rec = session.run(query, topic_name=topic_name).single()\n",
    "            return rec[\"branch_name\"] if rec else None\n",
    "\n",
    "    def list_all_topics(self) -> List[str]:\n",
    "        query = \"MATCH (t:Topic) RETURN t.name AS name ORDER BY t.name\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query)\n",
    "            return [record[\"name\"] for record in result]\n",
    "\n",
    "    def fetch_all_lesson_embeddings(self) -> List[Dict]:\n",
    "        cypher = \"\"\"\n",
    "        MATCH (t:Topic)-[:HAS_LESSON]->(l:Lesson)\n",
    "        WHERE l.vector_embedding IS NOT NULL\n",
    "        RETURN t.name AS topic, l.title AS lesson, l.vector_embedding AS embedding\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            records = session.run(cypher)\n",
    "            return [record.data() for record in records]\n",
    "\n",
    "    def fetch_lesson_images(self, lesson_title: str) -> List[Dict]:\n",
    "        cypher = \"\"\"\n",
    "        MATCH (l:Lesson {title: $title})-[:HAS_IMAGE]->(img:Image)\n",
    "        RETURN img.name AS name, img.caption AS caption, img.page AS page\n",
    "        ORDER BY img.page\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            return session.run(cypher, title=lesson_title).data()\n",
    "\n",
    "def render_pdf(mem: SessionMemory, outfile: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Renders a PDF report including summary, Q&A, quiz, and final feedback.\n",
    "    Handles images and styled text (like **bold**) for better layout.\n",
    "    \"\"\"\n",
    "    from reportlab.platypus import Paragraph\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.lib.enums import TA_RIGHT\n",
    "    from reportlab.lib import colors\n",
    "\n",
    "    c = pdf_canvas.Canvas(str(outfile), pagesize=A4)\n",
    "    w, h = A4\n",
    "    margin_top, margin_bottom = 40, 40\n",
    "    leading = 20\n",
    "    y = h - margin_top\n",
    "\n",
    "    # Styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    rtl_style = ParagraphStyle(\n",
    "        name=\"RTL\",\n",
    "        fontName=ARABIC_FONT_NAME,\n",
    "        fontSize=13,\n",
    "        leading=leading,\n",
    "        alignment=TA_RIGHT,\n",
    "        textColor=colors.black,\n",
    "    )\n",
    "    \n",
    "    def new_page():\n",
    "        nonlocal y\n",
    "        c.showPage()\n",
    "        y = h - margin_top\n",
    "\n",
    "    def wrap_line(line, width=80):\n",
    "        words, buf, out = line.split(), [], []\n",
    "        for w_ in words:\n",
    "            if sum(len(x) for x in buf) + len(w_) + len(buf) > width:\n",
    "                out.append(\" \".join(buf))\n",
    "                buf = [w_]\n",
    "            else:\n",
    "                buf.append(w_)\n",
    "        if buf:\n",
    "            out.append(\" \".join(buf))\n",
    "        return out\n",
    "\n",
    "    def draw_text(line: str, font=ARABIC_FONT_NAME, fsize=13):\n",
    "        nonlocal y\n",
    "        if y - leading < margin_bottom:\n",
    "            new_page()\n",
    "        # Check for markdown-style bold and set font weight\n",
    "        bold_parts = re.findall(r\"\\*\\*(.*?)\\*\\*\", line)\n",
    "        if bold_parts:\n",
    "            parts = re.split(r\"(\\*\\*.*?\\*\\*)\", line)\n",
    "            for part in parts:\n",
    "                if part.startswith(\"**\") and part.endswith(\"**\"):\n",
    "                    text = rtl(strip_unsupported(part[2:-2]))\n",
    "                    c.setFont(font + \"-Bold\", fsize)  # You need the bold variant installed\n",
    "                else:\n",
    "                    text = rtl(strip_unsupported(part))\n",
    "                    c.setFont(font, fsize)\n",
    "                c.drawRightString(w - margin_bottom, y, text)\n",
    "                y -= leading\n",
    "        else:\n",
    "            c.setFont(font, fsize)\n",
    "            c.drawRightString(w - margin_bottom, y, rtl(strip_unsupported(line)))\n",
    "            y -= leading\n",
    "\n",
    "\n",
    "    def draw_image(img_path: str, alt: str):\n",
    "        nonlocal y\n",
    "        if img_path.startswith(\"assets/book_images/\"):\n",
    "            img_path = img_path.replace(\"assets/book_images/\", \"\")\n",
    "        full_path = f\"{IMG_DIR}/{img_path}\".replace(\" \", \"\")\n",
    "\n",
    "        try:\n",
    "            im = Image.open(full_path)\n",
    "        except FileNotFoundError:\n",
    "            draw_text(f\"[صورة غير موجودة] {alt}\")\n",
    "            return\n",
    "\n",
    "        iw, ih = im.size\n",
    "        scale = min(MAX_IMG_W / iw, MAX_IMG_H / ih, 1.0)\n",
    "        dw, dh = iw * scale, ih * scale\n",
    "\n",
    "        if y - dh - leading < margin_bottom:\n",
    "            new_page()\n",
    "\n",
    "        c.drawInlineImage(full_path, w - margin_bottom - dw, y - dh, width=dw, height=dh)\n",
    "        y -= dh + leading // 2\n",
    "        draw_text(alt, font=ARABIC_FONT_NAME, fsize=11)\n",
    "\n",
    "    def draw_rich_block(title: str, raw_md: str | list):\n",
    "        nonlocal y\n",
    "        draw_text(title, fsize=15)\n",
    "        c.setStrokeColorRGB(0.6, 0.6, 0.6)\n",
    "        c.line(margin_bottom, y + 6, w - margin_bottom, y + 6)\n",
    "        y -= leading // 2\n",
    "\n",
    "        if isinstance(raw_md, list):\n",
    "            lines = []\n",
    "            for slide in raw_md:\n",
    "                if isinstance(slide, dict):\n",
    "                    text = slide.get(\"text\", \"\")\n",
    "                    text = re.sub(r\"<b>(.*?)</b>\", r\"**\\1**\", text)\n",
    "                    image = slide.get(\"image\")\n",
    "                    lines.extend(text.splitlines())\n",
    "                    if image:\n",
    "                        lines.append(f\"![]({image})\")\n",
    "            raw_md = \"\\n\".join(lines)\n",
    "\n",
    "        for paragraph in raw_md.splitlines():\n",
    "            paragraph = paragraph.strip()\n",
    "            if not paragraph:\n",
    "                y -= leading // 2\n",
    "                continue\n",
    "\n",
    "            m = MD_IMG.fullmatch(paragraph)\n",
    "            if m:\n",
    "                alt, path = m.group(1).strip(), m.group(2).strip()\n",
    "                draw_image(path, alt)\n",
    "                continue\n",
    "\n",
    "            idx = 0\n",
    "            for m in MD_IMG.finditer(paragraph):\n",
    "                pre = paragraph[idx:m.start()].rstrip()\n",
    "                if pre:\n",
    "                    for l in wrap_line(pre):\n",
    "                        draw_text(l)\n",
    "                alt, path = m.group(1).strip(), m.group(2).strip()\n",
    "                draw_image(path, alt)\n",
    "                idx = m.end()\n",
    "            tail = paragraph[idx:].rstrip()\n",
    "            if tail:\n",
    "                for l in wrap_line(tail):\n",
    "                    draw_text(l)\n",
    "\n",
    "        y -= leading // 2\n",
    "\n",
    "    draw_text(\"📗 تقرير التعلّم\", fsize=20)\n",
    "    y -= leading\n",
    "\n",
    "    if \"chapter_summary\" in mem:\n",
    "        for chapter in mem[\"chapter_summary\"]:\n",
    "            title = chapter.get(\"title\", \"ملخّص الدرس\")\n",
    "            slides = chapter.get(\"slides\", [])\n",
    "            draw_rich_block(title, slides)\n",
    "\n",
    "    if \"qa_history\" in mem:\n",
    "        lines = []\n",
    "        for q, a in mem[\"qa_history\"]:\n",
    "            lines.append(f\"❓ {q}\\n📥 {a}\\n\")\n",
    "        draw_rich_block(\"الأسئلة و الأجوبة:\", \"\\n\".join(lines))\n",
    "\n",
    "    if \"quiz_log\" in mem:\n",
    "        q_lines = []\n",
    "        for idx, qd in enumerate(mem[\"quiz_log\"], 1):\n",
    "            q_text = qd.get(\"q\", \"\")\n",
    "            correct = qd.get(\"a\", \"?\")\n",
    "            q_lines.append(f\"{idx}) {q_text}\")\n",
    "            if qd[\"type\"] == \"mc\":\n",
    "                q_lines.append(\"   \" + \"، \".join(qd[\"options\"]))\n",
    "            q_lines.append(f\"   الإجابة الصحيحة: {correct}\\n\")\n",
    "        draw_rich_block(\"تفاصيل الاختبار:\", \"\\n\".join(q_lines))\n",
    "\n",
    "    score_txt = \"\"\n",
    "    if \"feedback_note\" in mem:\n",
    "        score_txt += mem[\"feedback_note\"]\n",
    "    if score_txt:\n",
    "        draw_rich_block(\"التقييم النهائي:\", score_txt)\n",
    "\n",
    "    c.save()\n",
    "    return outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T13:36:29.800950Z",
     "iopub.status.busy": "2025-06-12T13:36:29.800684Z",
     "iopub.status.idle": "2025-06-12T13:36:29.804819Z",
     "shell.execute_reply": "2025-06-12T13:36:29.804001Z",
     "shell.execute_reply.started": "2025-06-12T13:36:29.800910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyCxEU-6lVV1PREeEy29pfDdFwmuKcTU7mc\"\n",
    "\n",
    "URI = \"neo4j+s://3e253ce0.databases.neo4j.io\"\n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"eMH4uA1k--yp1Ugwev9vXbXPnzVVo3QVaRLZ7Sh4_gU\"  # ← change to your Neo4j password\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Assistant (Arabic/Tunisian) \n",
    "- **OCR & PDF Retrieval:** Processes Arabic PDFs with semantic embedding and context-aware retrieval using HuggingFace models and Chroma.\n",
    "- **CrewAI Agents:** Specialized Tunisian-Arabic agents for summarization, Q&A, quizzes, and personalized feedback.\n",
    "- **FastAPI Integration:** RESTful endpoints for interactive lessons, quizzes, and automated PDF report generation tailored for students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T13:36:39.026894Z",
     "iopub.status.busy": "2025-06-12T13:36:39.026620Z",
     "iopub.status.idle": "2025-06-12T13:41:25.359889Z",
     "shell.execute_reply": "2025-06-12T13:41:25.359355Z",
     "shell.execute_reply.started": "2025-06-12T13:36:39.026871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 13:36:58.788624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749735419.036328     890 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749735419.113886     890 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/761k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/808 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached OCR data from /kaggle/working/-     .pdf.json\n",
      "Downloading ngrok ...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_890/1638984576.py:104: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  QA_MEMORY = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://a91c-34-90-35-83.ngrok-free.app\" -> \"http://localhost:8000\"       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [890]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"OPTIONS /summary HTTP/1.1\" 200 OK\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "إنتي معلّم/ة تونسي/ة؛ هدفك تبسّط محور “التنقل” من فرع “أحياء” لتلميذ في\n",
      "السنة الرابعة ابتـدائي. ركّز على الفهم، ربط الأفكار بحياتو اليومية،\n",
      "وتنويع الأمثلة.\n",
      "\n",
      "المعطيات قدامك:\n",
      "┌─ الدروس الفرعيّة:\n",
      "• أنماط التنقل عند الحيوان\n",
      "• تكيف العضو مع نمط التنقل\n",
      "\n",
      "┌─ مقتطفات من الكتاب (تستعملها كان تحب تقتبس جملة ولا توضيح):\n",
      "التنقل عند ا لحيوان ‏ ب\n",
      "\n",
      "الموضوع : تكيف العضو مح نيط التنقل\n",
      "\n",
      "الهدف : أتِبَيَنْ تكيق العضو مع نمط التنقل\n",
      "\n",
      "©\n",
      "- أرسم الطذرف الأمامي والخلفي لأرنب ثم أقارن بينهما\n",
      "- أبحث عن طول قفزة حيوان يتنقل قفزا ثم أقارن بين طول قفزة الحيوانوطول جسمه\n",
      "- أسمي الأعضاء التي تمك السمكة من المحافظة على توازنها أقنَاء السباحة\n",
      "- أرسم على كراسي جناحي طائر أثناء الإقلاع . التحليق » التزول\n",
      "- أفسر لماذا : يأخذ الجناحان شكلا معينا خلال هذه المراحل.\n",
      "- تتميرٌ الحيوانات التي تتنقل عن طريق العدو بآتساع القفص الصدري وقوة عضلاتها\n",
      "إلى جانب طول القوائم وانتصابها\n",
      "\n",
      "- يفوق طول قفزة الأرنب 6 مرات طول جسمها\n",
      "\n",
      "- أثناء السباحة يدفع تحرك الذنب السمكة إلى الأمام بينما تساعد بقية الزعانف على\n",
      "\n",
      "توازن السمكة والتحكم في تحركها\n",
      "\n",
      "7 تعلمت :\n",
      "\n",
      "أتأمل الجدول وأستعين به لكتابة أهم ما توصلت إليه في نهاية الدّرس (على كراسي)\n",
      "\n",
      "  \n",
      "\n",
      "خا 2\n",
      "الموضوع : أنياط التنقل عنر الحيوات (برا - بحرا - جوا)\n",
      "الهدف : أتَبَين أتماط تتقل تقل الحيو نات\n",
      "\n",
      "© -ب. -ابحث في موسوعتك العلمية أو أبحر في عالم الأنترنات للبحث عن أسماء حيوانات برية\n",
      "وبحرية وطيور نادرة واذكر أنماط تنقلها والأوساط التي تتنقل فيها\n",
      "\n",
      " \n",
      "\n",
      "هه\n",
      "\n",
      "9 كه\n",
      "\n",
      "تأمل هذه الصورة\n",
      "كيف يتم تنقل الحمامة و البطة ؟\n",
      "\n",
      "لا ام\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "6\n",
      "التنقل عند ا لحيوان ‏ ب\n",
      "\n",
      "الموضوع : تكيف العضو مح نيط التنقل\n",
      "\n",
      "الهدف : أتِبَيَنْ تكيق العضو مع نمط التنقل\n",
      "\n",
      "©\n",
      "- أرسم الطذرف الأمامي والخلفي لأرنب ثم أقارن بينهما\n",
      "- أبحث عن طول قفزة حيوان يتنقل قفزا ثم أقارن بين طول قفزة الحيوانوطول جسمه\n",
      "- أسمي الأعضاء التي تمك السمكة من المحافظة على توازنها أقنَاء السباحة\n",
      "- أرسم على كراسي جناحي طائر أثناء الإقلاع . التحليق » التزول\n",
      "- أفسر لماذا : يأخذ الجناحان شكلا معينا خلال هذه المراحل.\n",
      "ككاى الكلمية\n",
      "لا -أحيواء\n",
      "-الحواس والوقاية من الأمراض\n",
      "\n",
      "-التنقل\n",
      "\n",
      "-التغذية\n",
      "-التكاثر والنمو\n",
      "-التنفس\n",
      "الدرس\n",
      "- الحواس وأعضاء الحس' 0غ\n",
      "وظائف الجلد ووقايته همه مهمه ع مم 9\n",
      "التأثيرات السلبية على حاستي السمع والابصار ممم 120\n",
      "حماية السمع والابصار من المؤثرات المزعجة 00\n",
      "تأثير مرض الزكام علي الجسم 0\n",
      "حصة إدماج ممم مه ممم 2200000\n",
      "حصة دعم 211100\n",
      "أنماط التنقل عند الحيوان 2\n",
      "تكيف العضو مع نمط التنقل 000\n",
      "حصة تقييم 00\n",
      "- مصادر الأغذية ممع ممع مع عم عم ل 3660\n",
      "- مسار الأغذية وتحولها داخل الأنيوب لحيوان عاشب 000 41\n",
      "- أنواع الأسنان ووظائفها 0\n",
      "- وقاية الأسنان 1غ\n",
      "- حصة إدماج ممم 52\n",
      "- حصة تقييم ع ع م ع مم م م 54\n",
      "- التكاثر دون يذور 1010101111100\n",
      "- حصة تقييم 101100\n",
      "- أعضاء التنفس لدى بعض الحيوانات مم مم مم م 6200\n",
      "- أعضاء التنفس لدى بعض الحيوانات : الرئتان عند الخروف 66\n",
      "- أعضاء التنفس لدى بعض الحيوانات : الغلاصم عند السمكة 0\n",
      "- حصة إدماج 0غ\n",
      "\n",
      "- حصة تقييم ل 0000|\n",
      "الستد :\n",
      "فتحت التلفاز وجلست أتفريج على برنامج الصور المتحركة فكان أبطال الحلقة حيوانات. في\n",
      "نهاية البرنامج وجدتني أطرح على نفسي عدة تساؤلات فهلاً ساعدتني على الإجابة عنها\n",
      "\n",
      "  \n",
      "\n",
      "أتأمل المخطط وأشطب العنصر الزائد\n",
      "\n",
      "خصائص الأعضاء المساعدة على التنقل برأ\n",
      "\n",
      "أقرأ ثم أعلل :\n",
      "تستطيع السلّحفاة البحرية التنقل في البّر لماذا ؟\n",
      "تتميز الحيوانات التي تتنقل قفزا بطول القوائم الخلفية وقصر القوائم الأمامية\n",
      "\n",
      "أصلح المعلومة الخاطئة :\n",
      "- لبعض الحيوانات أعضاء تساعدها على التكيف مع نمط واحد من التنقل فقط\n",
      "- لكل الحيوانات أعضاء تساعدها على التكيف مع نمطين مختلفين من التنقل\n",
      "- تتنقل بعض الحيوانات المائية قريبا من سطح الماء.\n",
      "\n",
      "┌─ مجموعة تصاور مرتبطة (اختياري تستعمل بعضها):\n",
      "درس «أنماط التنقل عند الحيوان» – التصاور:\n",
      "* [صورة لبطتين على نهر.](page_26_img_23.jpeg)\n",
      "* [صورة لحمامة ذات ألوان متعددة.](page_26_img_24.jpeg)\n",
      "* [صورة لسَمَكَ مُلَوّن.](page_27_img_31.jpeg)\n",
      "* [طائر أبيض رمادي يطير فوق ماء أزرق.](page_27_img_32.jpeg)\n",
      "* [صورة تمثل تمساحًا أخضر.](page_27_img_25.jpeg)\n",
      "* [صورة لطائر البط.](page_27_img_26.jpeg)\n",
      "* [نموذجٌ ثلاثي الأبعاد لنمر مُصوّر.](page_27_img_29.jpeg)\n",
      "* [صورة دجاجة حمراء.](page_27_img_28.jpeg)\n",
      "* [صورة لطائر نعامة في وضعية سير.](page_27_img_27.jpeg)\n",
      "* [صورة لطائر بني اللون على غصن.](page_27_img_30.jpeg)\n",
      "* [صورة لـ كوالة برتقالية اللون.](page_27_img_33.jpeg)\n",
      "* [صورة لصقر بالجناحين مفتوحين.](page_29_img_34.jpeg)\n",
      "* [أرنب برّي يقفز بسرعة.](page_29_img_35.jpeg)\n",
      "\n",
      "درس «تكيف العضو مع نمط التنقل» – التصاور:\n",
      "* [صورة حصان برتقالي اللون يمشي.](page_31_img_37.jpeg)\n",
      "* [أرنبٌ يركضُ بسرعةٍ.](page_31_img_38.jpeg)\n",
      "* [صورةٌ لنُسرٍ جارحٍ مُرسمٍ.](page_31_img_36.jpeg)\n",
      "* [صورة لطائر بطة ملونة.](page_32_img_41.jpeg)\n",
      "* [صورة لنسر جناحيه مفتوحة.](page_32_img_43.jpeg)\n",
      "* [ساقا حصان في وضعية المشي.](page_32_img_42.jpeg)\n",
      "* [سمكة برّاقة ذات ألوان مائية.](page_32_img_40.jpeg)\n",
      "* [سمكة صفراء ذات زعانف حمراء.](page_32_img_39.jpeg)\n",
      "* [صورة توضيحية لعضلات الكوع.](page_33_img_44.jpeg)\n",
      "* [صورة توضيحية لعضلات الساق.](page_33_img_45.jpeg)\n",
      "\n",
      "\n",
      "طريقة العمل المطلوبة:\n",
      "1) إفتتاحيّة صغيرة بالدارجة (سطرين إلى ٣ سطور) تعرّف فيها بالمحور\n",
      "   ولماذا يهمّ التلميذ في حياتو.\n",
      "2)  بعد الإفتتاحية، اعمل لكل درس فرعي الي عندك :\n",
      "    • اشرح الفكرة الرئيسية بعبارة مبسّطة.\n",
      "    • أعط مثالًا واقعيًا من حياة الطفل (الدار، الحومة، الطبيعة…).\n",
      "    • إذا عندك صورة توضّح الفكرة، أدرجها في مصفوفة الشرائح بهذا الشكل:\n",
      "      ![وصف مختصر](assets/book_images/page_6_img_0.jpeg)\n",
      "        ↳ الصور اختيارية، ولا تستخدم أكثر من ٣ صور في كامل الملخص.\n",
      "3) أختم بسطر يُلخّص الدرس وقلوا اذا عندك سؤال انك موجود في خانة اسئلني.\n",
      "\n",
      "تنبيهات أسلوبيّة:\n",
      "- اكتب باللهجة التونسية الخفيفة، جُمَل قصيرة، مفردات مألوفة.\n",
      "- استخدم أفعال أمر إيجابية: «جرّب»، «ركّز»، «لاحظ».\n",
      "- لا تذكر أرقام الصفحات ولا أسماء الملفات داخل النص (إلا في صيغة الماركداون ![alt](path)).\n",
      "\n",
      "⬇️ **المخرجات يجب أن تكون JSON فقط، مطابقًا لهذا الهيكل بالضبط** ⬇️\n",
      "{\n",
      "  \"title\": \"درس عن التنقل\",\n",
      "  \"slides\": [\n",
      "    { \"number\": \"1\", \"text\": \"شرح الفكرة الأولى هنا\", \"image\": \"assets/book_images/page_6_img_0.jpeg\" },\n",
      "    { \"number\": \"2\", \"text\": \"شرح الفكرة الثانية هنا\"},\n",
      "    { \"number\": \"3\", \"text\": \"…\", \"image\": \"assets/book_images/page_6_img_2.jpeg\" },\n",
      "    //  أضف شرائح أخرى بنفس البنية؛ إذا لا توجد صورة، لا تضف حقل الصورة كالمثال رقم اثنان\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /summary HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"OPTIONS /quiz HTTP/1.1\" 200 OK\n",
      "{'subject': 'أحياء', 'module': 'التنفس'}\n",
      "التنفس\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /quiz HTTP/1.1\" 200 OK\n",
      "{'module': 'التنفس', 'num_mc': 6, 'num_tf': 4}\n",
      "التنفس\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /quiz HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"OPTIONS /qa HTTP/1.1\" 200 OK\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:40:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /qa HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"OPTIONS /finish HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:41:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 [report] Entering report_endpoint\n",
      "['ملخّص الدرس:\\n[{\"title\": \"درس عن التنقل\", \"slides\": [{\"number\": \"1\", \"text\": \"في درسنا اليوم، باش نشوفو كيفاش الحيوانات تتنقل وتتحرك. كل حيوان عندو طريقة خاصة بيه باش يمشي، يطير، يعوم... كيما أنت عندك ساقين تمشي بيهم!\", \"image\": \"assets/book_images/page_27_img_28.jpeg\"}, {\"number\": \"2\", \"text\": \"<b>أنماط التنقل عند الحيوان:</b> الحيوانات تتنقل بطرق مختلفة: فما اللي يمشي على ساقيه كيما الكلب، واللي يطير بجناحاته كيما العصفور، واللي يعوم بزعانفه كيما الحوت. كل حيوان وعندو طريقتو!\", \"image\": \"assets/book_images/page_26_img_24.jpeg\"}, {\"number\": \"3\", \"text\": \"مثال: الحمامة تطير في الجو، والبطة تعوم في الماء. النمر يجري في الغابة، والسمكة تسبح في البحر. لاحظ الحيوانات اللي تراها في حومتك، كيفاش تتحرك؟\", \"image\": \"assets/book_images/page_26_img_23.jpeg\"}, {\"number\": \"4\", \"text\": \"<b>تكيف العضو مع نمط التنقل:</b> ربي سبحانه عطى لكل حيوان أعضاء تساعدو على التنقل بطريقة معينة. مثلاً، الأرنب عندو ساقين قويين باش يقفز، والسمكة عندها زعانف باش تعوم.\", \"image\": \"assets/book_images/page_31_img_38.jpeg\"}, {\"number\": \"5\", \"text\": \"مثال: ساقين الحصان قويين باش يجري بسرعة، وجناحات النسر كبار باش يطير في السماء. لاحظ كيفاش شكل جسم الحيوان يساعدو على الحركة.\", \"image\": \"assets/book_images/page_31_img_37.jpeg\"}, {\"number\": \"6\", \"text\": \"في النهاية، تعلمنا أنو كل حيوان عندو طريقة خاصة بيه باش يتنقل، والأعضاء متاعو متكيفة مع طريقة التنقل هذيكا. إذا عندك أي سؤال، أنا موجود!\"}]}]', 'الأسئلة و الأجوبة:\\n❓ فسرلي انواع التنقل\\n📥 باش نفسّرلك أنواع التنقل يا وليدي، يلزم نفهمو أول حاجة شنوة معناها \"تنقل\". التنقل هو كيفاش الكائن الحي يتحرك من بلاصة لبلاصة. تخيل روحك تلعب بالكورة، تجري باش توصلها، هذاكا هو التنقل!\\n\\nعندنا أنواع مختلفة متاع تنقل عند الحيوانات، كل حيوان وكيفاش يتحرك:\\n\\n*   **المشي والجري:** كيما أنت تمشي وتجري، فما حيوانات أخرى تعمل كيفك، كيما الكلب والقطوس والحصان. يستعملوا ساقيهم باش يمشيو ويجرو.\\n*   **القفز:** كيما الكنغر والأرنب، يستعملوا ساقيهم الخلفية القوية باش يقفزوا وينطوا. تخيل روحك تلعب \"الحجلة\"، هذاكا هو القفز!\\n*   **الزحف:** كيما الثعبان، ماعندوش ساقين، يزحف على بطنه باش يتحرك.\\n*   **السباحة:** كيما الحوت والبطة، يستعملوا زعانفهم وأرجلهم باش يعوموا في الماء.\\n*   **الطيران:** كيما الطيور والفراشات، يستعملوا أجنحتهم باش يطيروا في السماء.\\n\\nكل حيوان عنده طريقة التنقل الخاصة بيه اللي تساعده باش يعيش وياكل ويهرب من الخطر. فهمت يا وليدي؟']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /finish HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"GET /reports/session_report.pdf HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [890]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Learning Assistant (Arabic/Tunisian) – CLI + PDF + FastAPI + Quiz\n",
    "Refactored for efficiency: single retriever/LLM/agents initialization\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Learning Assistant (Arabic/Tunisian) – CLI + PDF + FastAPI + Quiz\n",
    "Refactored for efficiency: single retriever/LLM/agents initialization\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, re, os, math\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple, Type  # Added Type import\n",
    "\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "\n",
    "from crewai import Agent, Crew, Task, LLM\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "import nest_asyncio, uvicorn\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# ─────────────────────── Paths & Constants ─────────────────────────\n",
    "PDF_PATH  = Path(\"/kaggle/input/i9adh-3elmi-sana-4/-     .pdf\")\n",
    "IMG_DIR   = Path(\"/kaggle/input/book-images\")\n",
    "SAVE_PATH = Path(\"/kaggle/working/lessons\")\n",
    "SAVE_PATH.mkdir(exist_ok=True, parents=True)\n",
    "emb    = HuggingFaceEmbeddings(model_name=\"Omartificial-Intelligence-Space/GATE-AraBert-v1\")\n",
    "cross  = HuggingFaceCrossEncoder(model_name=\"Omartificial-Intelligence-Space/ARA-Reranker-V1\")\n",
    "ARABIC_FONT_PATH = \"/kaggle/input/nottoooo/NotoNaskhArabic-Regular.ttf\"     # ← change if needed\n",
    "ARABIC_FONT_NAME = \"NotoArabic\"\n",
    "ARABIC_FONT_PATH_bold = \"/kaggle/input/text-bold/NotoNaskhArabic-Bold.ttf\"     # ← change if needed\n",
    "ARABIC_FONT_NAME_bold = \"NotoArabic-Bold\"\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────── Utilities ─────────────────────────\n",
    "class SessionMemory(dict):\n",
    "    def log(self, key: str, value: Any):\n",
    "        self.setdefault(key, []).append(value)\n",
    "\n",
    "\n",
    "def rtl(txt: str) -> str:\n",
    "    return get_display(arabic_reshaper.reshape(txt))\n",
    "\n",
    "\n",
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    dot = sum(x*y for x,y in zip(a,b))\n",
    "    n1  = math.sqrt(sum(x*x for x in a))\n",
    "    n2  = math.sqrt(sum(y*y for y in b))\n",
    "    return dot/(n1*n2) if n1 and n2 else 0.0\n",
    "\n",
    "def _clean_user_question(raw: str) -> str:\n",
    "    l = raw.strip().lower()\n",
    "    return raw.split(':',1)[1].strip() if l.startswith(('سؤال:','qa:')) else raw.strip()\n",
    "\n",
    "\n",
    "def _clean_json(text: str) -> str:\n",
    "    t = re.sub(r'```[a-zA-Z]*\\n?','', text).strip()\n",
    "    return t.strip('`').strip()\n",
    "\n",
    "\n",
    "def parse_quiz_json(raw: str) -> Any:\n",
    "    cleaned = _clean_json(raw).replace(\"'\", '\"')\n",
    "    return json.loads(cleaned)\n",
    "\n",
    "# ───────────────────── Retriever & LLM Setup ─────────────────────\n",
    "def build_retriever(\n",
    "    pdf_path: Path,\n",
    "    emb=emb,\n",
    "    cross=cross,\n",
    "    k_fetch: int = 8,\n",
    "    k_rerank: int = 3\n",
    ") -> ContextualCompressionRetriever:\n",
    "    docs   = load_arabic_pdf(pdf_path)\n",
    "    splits = SemanticChunker(emb).split_documents(docs)\n",
    "    vect   = Chroma.from_documents(splits, emb)\n",
    "    base   = vect.as_retriever(search_kwargs={\"k\": k_fetch})\n",
    "    comp = CrossEncoderReranker(model=cross, top_n=k_rerank)\n",
    "    return ContextualCompressionRetriever(base_retriever=base, base_compressor=comp)\n",
    "\n",
    "# Initialize once\n",
    "global_mem = SessionMemory()  # <== added\n",
    "RETRIEVER = build_retriever(PDF_PATH,emb,cross)\n",
    "LLM_MODEL = LLM(model=\"gemini/gemini-2.0-flash\", temperature=0.5, max_tokens=\"4000\")\n",
    "QA_MEMORY = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# ───────────────────── ChapterRetriever Tool ─────────────────────\n",
    "class ChapterRetrieverInput(BaseModel):\n",
    "    query: str = Field(..., description=\"السؤال أو عنوان الدرس\")\n",
    "\n",
    "class ChapterRetrieverTool(BaseTool):\n",
    "    name: str = \"chapter_retriever\"  # Annotated with type\n",
    "    description: str = \"يجيب مقاطع من الكتاب حسب السؤال أو عنوان الدرس.\"  # Annotated with type\n",
    "    args_schema: Type[BaseModel] = ChapterRetrieverInput  # Annotated with Type\n",
    "\n",
    "    def __init__(self, retriever: ContextualCompressionRetriever):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, '_retriever', retriever)\n",
    "\n",
    "    def _run(self, query: str, **kwargs) -> List[str]:\n",
    "        return [d.page_content for d in self._retriever.invoke(query)]\n",
    "\n",
    "# Rebuild Pydantic model to finalize annotations\n",
    "ChapterRetrieverTool.model_rebuild()\n",
    "\n",
    "# Instantiate tool and agents once\n",
    "tool = ChapterRetrieverTool(RETRIEVER)\n",
    "\n",
    "SUMMARY_AGENT = Agent(\n",
    "    role=\"ملخّص الدرس\",\n",
    "    goal=\"ملخّص ساهل بالدارجة\",\n",
    "    backstory=\"معلّمة توضّح الدروس.\",\n",
    "    llm=LLM_MODEL,\n",
    "    tools=[tool],\n",
    "    verbose=False\n",
    ")\n",
    "QA_AGENT = Agent(\n",
    "    role=\"معلّم يجاوب على الأسئلة\",\n",
    "    goal=\"يقدم إجابات دقيقة ومبسّطة على أسئلة التلميذ.\",\n",
    "    backstory=\"معلّم تونسي صبور.\",\n",
    "    llm=LLM_MODEL,\n",
    "    tools=[tool],\n",
    "    memory=QA_MEMORY,\n",
    "    verbose=False\n",
    ")\n",
    "QUIZ_AGENT = Agent(\n",
    "    role=\"صانع الامتحانات\",\n",
    "    goal=\"يعمل أسئلة بسيطة ويصحّحها بناءً على المحور المحدّد\",\n",
    "    backstory=\"يحب النجوم الذهبية.\",\n",
    "    llm=LLM_MODEL,\n",
    "    tools=[tool],\n",
    "    verbose=False\n",
    ")\n",
    "FEEDBACK_AGENT = Agent(\n",
    "    role=\"معدّ التقرير\",\n",
    "    goal=\"يكتب تقرير PDF مشجّع\",\n",
    "    backstory=\"أخصّائي متابعة تعلم.\",\n",
    "    llm=LLM_MODEL,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# ──────────────────── Helper Functions ─────────────────────────\n",
    "def retrieve_context(topic: str, kg) -> Tuple[str, str]:\n",
    "    lessons = kg.get_lessons_for_topic(topic)\n",
    "    text_chunks = []\n",
    "    images_blocks = []\n",
    "    for ld in lessons:\n",
    "        text_chunks.extend(tool.run(ld['title']))\n",
    "        pics = kg.fetch_lesson_images(ld['title'])\n",
    "        if pics:\n",
    "            md = \"\\n\".join(f\"* [{i['caption']}]({i['name']})\" for i in pics)\n",
    "            images_blocks.append(f\"درس «{ld['title']}» – التصاور:\\n{md}\\n\")\n",
    "    return \"\\n\".join(text_chunks[:30]), (\"\\n\".join(images_blocks) or \"ما ثـمّـة حتى تصاور.\")\n",
    "\n",
    "# ────────────────── Summary Generator ─────────────────────────\n",
    "def generate_summary_json(user_in: str, kg) -> dict:\n",
    "    m = re.match(r\"ملخص\\s+(?:محور\\s+)?(?P<topic>[\\u0600-\\u06FF ]+)\", user_in)\n",
    "    if not m:\n",
    "        raise ValueError(\"⚠️ لازم تذكر اسم المحور بعد كلمة «ملخص».\")\n",
    "    topic = m.group('topic').strip()\n",
    "    branch = kg.find_branch_for_topic(topic)\n",
    "    lessons_info = kg.get_lessons_for_topic(topic)\n",
    "    if not branch or not lessons_info:\n",
    "        raise LookupError(f\"⚠️ ما لقيتش المحور «{topic}» في الـ KG.\")\n",
    "\n",
    "    ctx_text, images_section = retrieve_context(topic, kg)\n",
    "    sub_lessons_md = \"\\n\".join(f\"• {ld['title']}\" for ld in lessons_info)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "إنتي معلّم/ة تونسي/ة؛ هدفك تبسّط محور “{topic}” من فرع “{branch}” لتلميذ في\n",
    "السنة الرابعة ابتـدائي. ركّز على الفهم، ربط الأفكار بحياتو اليومية،\n",
    "وتنويع الأمثلة.\n",
    "\n",
    "المعطيات قدامك:\n",
    "┌─ الدروس الفرعيّة:\n",
    "{sub_lessons_md}\n",
    "\n",
    "┌─ مقتطفات من الكتاب (تستعملها كان تحب تقتبس جملة ولا توضيح):\n",
    "{ctx_text}\n",
    "\n",
    "┌─ مجموعة تصاور مرتبطة (اختياري تستعمل بعضها):\n",
    "{images_section}\n",
    "\n",
    "طريقة العمل المطلوبة:\n",
    "1) إفتتاحيّة صغيرة بالدارجة (سطرين إلى ٣ سطور) تعرّف فيها بالمحور\n",
    "   ولماذا يهمّ التلميذ في حياتو.\n",
    "2)  بعد الإفتتاحية، اعمل لكل درس فرعي الي عندك :\n",
    "    • اشرح الفكرة الرئيسية بعبارة مبسّطة.\n",
    "    • أعط مثالًا واقعيًا من حياة الطفل (الدار، الحومة، الطبيعة…).\n",
    "    • إذا عندك صورة توضّح الفكرة، أدرجها في مصفوفة الشرائح بهذا الشكل:\n",
    "      ![وصف مختصر](assets/book_images/page_6_img_0.jpeg)\n",
    "        ↳ الصور اختيارية، ولا تستخدم أكثر من ٣ صور في كامل الملخص.\n",
    "3) أختم بسطر يُلخّص الدرس وقلوا اذا عندك سؤال انك موجود في خانة اسئلني.\n",
    "\n",
    "تنبيهات أسلوبيّة:\n",
    "- اكتب باللهجة التونسية الخفيفة، جُمَل قصيرة، مفردات مألوفة.\n",
    "- استخدم أفعال أمر إيجابية: «جرّب»، «ركّز»، «لاحظ».\n",
    "- لا تذكر أرقام الصفحات ولا أسماء الملفات داخل النص (إلا في صيغة الماركداون ![alt](path)).\n",
    "\n",
    "⬇️ **المخرجات يجب أن تكون JSON فقط، مطابقًا لهذا الهيكل بالضبط** ⬇️\n",
    "{{\n",
    "  \"title\": \"درس عن {topic}\",\n",
    "  \"slides\": [\n",
    "    {{ \"number\": \"1\", \"text\": \"شرح الفكرة الأولى هنا\", \"image\": \"assets/book_images/page_6_img_0.jpeg\" }},\n",
    "    {{ \"number\": \"2\", \"text\": \"شرح الفكرة الثانية هنا\"}},\n",
    "    {{ \"number\": \"3\", \"text\": \"…\", \"image\": \"assets/book_images/page_6_img_2.jpeg\" }},\n",
    "    //  أضف شرائح أخرى بنفس البنية؛ إذا لا توجد صورة، لا تضف حقل الصورة كالمثال رقم اثنان\n",
    "  ]\n",
    "}}\n",
    "\"\"\" \n",
    "    print(prompt)\n",
    "    task = Task(description=prompt, expected_output=\"json\", agent=SUMMARY_AGENT)\n",
    "    raw = Crew(agents=[SUMMARY_AGENT], tasks=[task], verbose=False).kickoff().raw\n",
    "    cleaned = _clean_json(raw)\n",
    "    start = cleaned.find('{'); end = cleaned.rfind('}')\n",
    "    if start<0 or end<0:\n",
    "        raise HTTPException(502, \"No JSON object found in LLM output\")\n",
    "    data = json.loads(cleaned[start:end+1])\n",
    "\n",
    "    filename = f\"{branch}_{topic}.json\".replace(' ', '_')\n",
    "    path = SAVE_PATH/filename\n",
    "    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    return {\"path\": f\"/lessons/{filename}\", \"data\": data}\n",
    "\n",
    "# ──────────────────── QA Handler ─────────────────────────────\n",
    "def handle_qa(question: str, kg,emb) -> str:\n",
    "    # 1) clean question\n",
    "    q = _clean_user_question(question)\n",
    "\n",
    "    q_emb = emb.embed_query(q)\n",
    "    all_lessons = kg.fetch_all_lesson_embeddings()\n",
    "    best_score, topic, lesson = max(\n",
    "        ((cosine_similarity(q_emb, e['embedding']), e['topic'], e['lesson']) for e in all_lessons),\n",
    "        default=(0.0, None, None)\n",
    "    )\n",
    "\n",
    "    # 3) load previous conversation\n",
    "    mem_vars = QA_MEMORY.load_memory_variables({})\n",
    "    history = mem_vars.get(\"chat_history\", \"\")\n",
    "\n",
    "    # 4) build prompt including memory\n",
    "    if best_score >= 0.25 and topic:\n",
    "        ctx_text, _ = retrieve_context(topic, kg)\n",
    "        sub_md = \"\\n\".join(f\"• {ld['title']}\" for ld in kg.get_lessons_for_topic(topic))\n",
    "        prompt = (\n",
    "            f\"المحادثة السابقة:\\n{history}\\n\\n\"\n",
    "            f\"أنت معلّم صبور ولطيف. عندك هذه السؤال من الطفل:\\n«{q}»\\n\\n\"\n",
    "            f\"درس “{lesson}” تحت محور “{topic}” هو الأنسب.\\n\"\n",
    "            f\"هذه قائمة الدروس:\\n{sub_md}\\n\\n\"\n",
    "            \"قدّم شرحًا تفصيليًا بعبارات بسيطة:\\n\"\n",
    "            \"- عرف المصطلح.\\n\"\n",
    "            \"- مثال من الحياة اليومية.\\n\"\n",
    "            \"- فسّر الخطوات الصعيبة كأنك تشرح لتلميذ في الرابعة.\\n\\n\"\n",
    "            \"ما تذكرش أرقام الصفحات. أجب بلهجة تونسيّة.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"المحادثة السابقة:\\n{history}\\n\\n\"\n",
    "            \"أنت معلّم صبور ولطيف. وصلتك هذه السؤال من الطفل:\\n\"\n",
    "            f\"«{q}»\\n\\n\"\n",
    "            \"قدّم شرحًا تفصيليًا بعبارات بسيطة:\\n\"\n",
    "            \"- عرف المصطلح.\\n\"\n",
    "            \"- مثال من الحياة اليومية.\\n\"\n",
    "            \"- فسّر الخطوات الصعيبة كأنك تشرح لتلميذ في الرابعة.\\n\\n\"\n",
    "            \"ما تذكرش أرقام الصفحات. أجب بلهجة تونسيّة.\"\n",
    "        )\n",
    "\n",
    "    # 5) run the agent\n",
    "    task = Task(description=prompt, expected_output=\"text\", agent=QA_AGENT)\n",
    "    answer = Crew(agents=[QA_AGENT], tasks=[task], verbose=False).kickoff().raw\n",
    "\n",
    "    # 6) save to memory\n",
    "    QA_MEMORY.save_context({\"user_input\": q}, {\"assistant_output\": answer})\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────── Quiz Generator ──────────────────────────\n",
    "def generate_quiz_json(module: str, kg, num_mc: int=6, num_tf: int=4) -> dict:\n",
    "    branch = kg.find_branch_for_topic(module)\n",
    "    lessons_info = kg.get_lessons_for_topic(module)\n",
    "    if not branch or not lessons_info:\n",
    "        raise LookupError(f\"⚠️ ما لقيتش المحور «{module}» في الـ KG.\")\n",
    "    ctx_text,_ = retrieve_context(module, kg)\n",
    "    sub_list = \"\\n\".join(f\"• {ld['title']} (pages {ld['start_page']}–{ld['end_page']})\" for ld in lessons_info)\n",
    "    prompt = (\n",
    "        f\"أنت صانع امتحانات لتلاميذ السنوات الابتدائية. \"\n",
    "        f\"أعِدْ JSON يحتوي على **{num_mc} أسئلة اختيار من متعدد** و**{num_tf} أسئلة صح/خطأ** \"\n",
    "        f\"عن محور «{module}» من فرع «{branch}». \"\n",
    "        f\"تأكد أن تُغطّي جميع الدروس الفرعية:\\n{sub_list}\\n\\n\"\n",
    "        f\"بعض مقتطفات من الكتاب:\\n{ctx_text}\\n\\n\"\n",
    "        \"هيكل JSON المطلوب:\\n\"\n",
    "        \"\"\"{\n",
    "  \"questions\": [\n",
    "    {\"type\":\"mc\",\"q\":\"...\", \"options\":[\"...\",\"...\",\"...\",\"...\"], \"a\":\"...\"},\n",
    "    {\"type\":\"tf\",\"q\":\"...\",\"options\":[\"صح\",\"خطأ\"] \"a\":\"صح\"}\n",
    "  ]\n",
    "}\"\"\"\n",
    "    )\n",
    "    task = Task(description=prompt, expected_output=\"json\", agent=QUIZ_AGENT)\n",
    "    raw = Crew(agents=[QUIZ_AGENT], tasks=[task], verbose=False).kickoff().raw\n",
    "    data = parse_quiz_json(raw)\n",
    "    return {\"module\": module, \"data\": data}\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"], allow_credentials=True\n",
    ")\n",
    "app.mount(\"/lessons\", StaticFiles(directory=str(SAVE_PATH)), name=\"lesson_files\")\n",
    "app.mount(\"/reports\", StaticFiles(directory=\"/kaggle/working\"), name=\"reports\")\n",
    "\n",
    "neo_kg   = Neo4jKG(URI, USER, PASSWORD)\n",
    "\n",
    "@app.post(\"/summary\")\n",
    "async def summary_endpoint(req: Request):\n",
    "    body = await req.json()\n",
    "    mod = body.get(\"module\", \"\").strip()\n",
    "    if not mod:\n",
    "        return JSONResponse({\"error\": \"module is required\"}, status_code=400)\n",
    "\n",
    "    try:\n",
    "        user_in = f\"ملخص محور {mod}\"\n",
    "        result  = generate_summary_json(user_in, neo_kg)\n",
    "        global_mem.log('chapter_summary', result['data'])\n",
    "        return JSONResponse(result)\n",
    "\n",
    "    except LookupError as e:                 # 👈  catches branch / lessons not found\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=404)\n",
    "\n",
    "    except Exception as e:\n",
    "        # log full traceback for yourself\n",
    "        import traceback, sys\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        return JSONResponse({\"error\": \"internal failure\"}, status_code=500)\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.post(\"/qa\")\n",
    "async def qa_endpoint(req: Request):\n",
    "    body     = await req.json()\n",
    "    question = body.get(\"question\", \"\").strip()\n",
    "    if not question:\n",
    "        return JSONResponse({\"error\": \"question is required\"}, status_code=400)\n",
    "\n",
    "    try:\n",
    "        answer = handle_qa(question, neo_kg,emb)\n",
    "        global_mem.log('qa_history', (question, answer))\n",
    "        return JSONResponse(answer)\n",
    "\n",
    "    except LookupError as e:\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=404)\n",
    "    except Exception as e:\n",
    "        return JSONResponse({\"error\": \"internal failure\", \"details\": str(e)}, status_code=500)\n",
    "\n",
    "@app.post(\"/quiz\")\n",
    "async def quiz_endpoint(req: Request):\n",
    "    \"\"\"\n",
    "    Expects JSON: { \"module\": \"اسم المحور\", \"num_mc\": 6, \"num_tf\": 4 }\n",
    "    \"\"\"\n",
    "    body    = await req.json()\n",
    "    print(body)\n",
    "    module  = body.get(\"module\", \"\").strip()\n",
    "    print(module)\n",
    "    num_mc  = int(body.get(\"num_mc\", 6))\n",
    "    num_tf  = int(body.get(\"num_tf\", 4))\n",
    "    if not module:\n",
    "        return JSONResponse({\"error\": \"module is required\"}, status_code=400)\n",
    "    try:\n",
    "        result = generate_quiz_json(module, neo_kg, num_mc=num_mc, num_tf=num_tf)\n",
    "        global_mem['quiz_log'] = result['data']['questions']\n",
    "        # Initialize quiz_results as placeholder\n",
    "        global_mem['quiz_results'] = {\"correct\": 0, \"incorrect\": len(result['data']['questions'])}\n",
    "        return JSONResponse(result)\n",
    "    except LookupError as e:\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=404)\n",
    "    except Exception as e:\n",
    "        return JSONResponse({\"error\": \"internal failure\", \"details\": str(e)}, status_code=500)\n",
    "@app.post(\"/finish\")\n",
    "async def finish():\n",
    "    print(\"🟢 [report] Entering report_endpoint\")\n",
    "\n",
    "    # build feedback prompt from session history\n",
    "    parts = []\n",
    "    if 'chapter_summary' in global_mem:\n",
    "        parts.append(\"ملخّص الدرس:\\n\" + json.dumps(global_mem['chapter_summary'], ensure_ascii=False))\n",
    "    if 'qa_history' in global_mem:\n",
    "        qa_lines = [f\"❓ {q}\\n📥 {a}\" for q, a in global_mem['qa_history']]\n",
    "        parts.append(\"الأسئلة و الأجوبة:\\n\" + \"\\n\".join(qa_lines))\n",
    "    if 'quiz_log' in global_mem and any('child' in q for q in global_mem['quiz_log']):\n",
    "        quiz_questions = global_mem.get('quiz_log', [])\n",
    "        if isinstance(quiz_questions, dict) and 'questions' in quiz_questions:\n",
    "            quiz_questions = quiz_questions['questions']\n",
    "        quiz_lines = [\n",
    "            f\"{i+1}) {q.get('q')} – إجابة صحيحة: {q.get('a')}\"\n",
    "            for i, q in enumerate(quiz_questions)\n",
    "        ]\n",
    "        parts.append(\"تفاصيل الاختبار:\\n\" + \"\\n\".join(quiz_lines))\n",
    "\n",
    "    print(parts)\n",
    "    fb_prompt = (\n",
    "                \"أنت أخصّائي متابعة تعلّم للأطفال. \"\n",
    "                \"بناءً على هذه المعلومات من جلستهم:\\n\\n\"\n",
    "                + \"\\n---\\n\".join(parts)\n",
    "                + \"\\n\\n\"\n",
    "                \"اكتب رسالة تشجيعية قصيرة باللهجة التونسية، تلخّص نجاحاتهم وتشجّعهم على الاستمرار في الدراسة.\"\n",
    "            )\n",
    "\n",
    "    fb_task = Task(\n",
    "        description=fb_prompt,\n",
    "        expected_output=\"رسالة تشجيعية\",\n",
    "        agent=FEEDBACK_AGENT,\n",
    "    )\n",
    "    fb_note = Crew(agents=[FEEDBACK_AGENT], tasks=[fb_task], verbose=False).kickoff().raw\n",
    "    global_mem[\"feedback_note\"] = fb_note\n",
    "\n",
    "    pdf_path = Path(\"/kaggle/working/session_report.pdf\")\n",
    "    render_pdf(global_mem, pdf_path)\n",
    "\n",
    "    # Return the direct URL to the static file!\n",
    "    return JSONResponse({\n",
    "        \"pdf_url\": \"/reports/session_report.pdf\"\n",
    "    })\n",
    "# ────────────────────────── Initialize & Run ─────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    os.environ['ngrok_authToken']='2yMaZ6btidIIiv3fwpkG287hAOT_2ezDgPqKcpGa2w9Z3WpxT'\n",
    "    conf.get_default().auth_token = os.environ[\"ngrok_authToken\"]\n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(\"Public URL:\", public_url)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7598875,
     "sourceId": 12071821,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7612236,
     "sourceId": 12092243,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7631814,
     "sourceId": 12120485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7634088,
     "sourceId": 12123918,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7634727,
     "sourceId": 12124915,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7639496,
     "sourceId": 12131394,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
