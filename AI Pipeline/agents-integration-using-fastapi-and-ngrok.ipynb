{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install crewai crewai-tools \\\n",
    "#     langchain langchain-community langchain-google-genai \\\n",
    "#     langchain-huggingface sentence-transformers chromadb \\\n",
    "#     google-generativeai duckduckgo-search reportlab \\\n",
    "#     pydantic \n",
    "# !pip install pymupdf pytesseract pillow\n",
    "# !apt-get update\n",
    "# !apt-get install -y tesseract-ocr tesseract-ocr-ara\n",
    "# !pip install arabic_reshaper\n",
    "# !pip install neo4j pyngrok\n",
    "# !pip install langchain_experimental\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Helpers**\n",
    "This Cell performs OCR on Arabic PDFs using PyMuPDF, Tesseract, and Pillow.\n",
    "It preprocesses and caches OCR data, manages Arabic text rendering with ReportLab and supports RTL formatting.\n",
    "Additionally, it interfaces with Neo4j for knowledge graph operations and generates PDF reports with images and text styling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T13:36:12.921584Z",
     "iopub.status.busy": "2025-06-12T13:36:12.921315Z",
     "iopub.status.idle": "2025-06-12T13:36:14.300079Z",
     "shell.execute_reply": "2025-06-12T13:36:14.299344Z",
     "shell.execute_reply.started": "2025-06-12T13:36:12.921560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from typing import Any, List, Tuple, Type  # Added Type import\n",
    "# ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ReportLab + RTL helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas as pdf_canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "ARABIC_FONT_PATH = \"/kaggle/input/nottoooo/NotoNaskhArabic-Regular.ttf\"     # ‚Üê change if needed\n",
    "ARABIC_FONT_NAME = \"NotoArabic\"\n",
    "ARABIC_FONT_PATH_bold = \"/kaggle/input/text-bold/NotoNaskhArabic-Bold.ttf\"     # ‚Üê change if needed\n",
    "ARABIC_FONT_NAME_bold = \"NotoArabic-Bold\"\n",
    "pdfmetrics.registerFont(TTFont(ARABIC_FONT_NAME, ARABIC_FONT_PATH))\n",
    "pdfmetrics.registerFont(TTFont(ARABIC_FONT_NAME_bold, ARABIC_FONT_PATH_bold))\n",
    "\n",
    "import re\n",
    "from PIL import Image   # Pillow is installed in Kaggle images\n",
    "\n",
    "IMG_DIR = \"/kaggle/input/book-images\"     # adjust if your folder differs\n",
    "MAX_IMG_W = 180                          # pixel width allowed on page\n",
    "MAX_IMG_H = 140                          # pixel height allowed on page\n",
    "\n",
    "MD_IMG = re.compile(r'!\\[(.*?)\\]\\((.*?)\\)')   # ![alt](path)\n",
    "\n",
    "\n",
    "def wrap_arabic(text: str, max_chars: int = 70) -> List[str]:\n",
    "    \"\"\"Naive word-wrap for Arabic lines.\"\"\"\n",
    "    words = text.split()\n",
    "    lines, buf = [], []\n",
    "    for w in words:\n",
    "        if sum(len(x) for x in buf) + len(w) + len(buf) > max_chars:\n",
    "            lines.append(\" \".join(buf))\n",
    "            buf = [w]\n",
    "        else:\n",
    "            buf.append(w)\n",
    "    if buf:\n",
    "        lines.append(\" \".join(buf))\n",
    "    return lines\n",
    "\n",
    "def strip_unsupported(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove any character that is not:\n",
    "      - Arabic letters (U+0600‚ÄìU+06FF)\n",
    "      - Basic Latin letters/digits/punctuation (U+0000‚ÄìU+007F)\n",
    "      - Common Arabic punctuation: ÿå ÿü ! - (and space)\n",
    "    This effectively strips emojis and other symbols that the Arabic font cannot render.\n",
    "    \"\"\"\n",
    "    # Allow U+0600..U+06FF (Arabic), U+0000..U+007F (Basic Latin),\n",
    "    # and the Arabic comma (U+060C) and question mark (U+061F) and exclamation (U+0021) and dash/hyphen.\n",
    "    return re.sub(r\"[^\\u0000-\\u007F\\u0600-\\u06FF\\u060C\\u061F\\u0021\\u002D\\s]\", \"\", text)\n",
    "\n",
    "# Set Tesseract language data path\n",
    "os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata/'\n",
    "class SessionMemory(dict):\n",
    "    def log(self, key: str, value: Any):\n",
    "        self.setdefault(key, []).append(value)\n",
    "\n",
    "def load_arabic_pdf(pdf_path, lang=\"ara\", batch_size=40, cache_dir=\"/kaggle/working/\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    pdf_name = os.path.basename(pdf_path)\n",
    "    cache_file = os.path.join(cache_dir, pdf_name + \".json\")\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached OCR data from {cache_file}\")\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_docs = json.load(f)\n",
    "        return [Document(page_content=d[\"page_content\"], metadata=d[\"metadata\"]) for d in raw_docs]\n",
    "\n",
    "    documents = []\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "\n",
    "        for start in range(0, total_pages, batch_size):\n",
    "            end = min(start + batch_size, total_pages)\n",
    "            print(f\"Processing pages {start+1} to {end}...\")\n",
    "\n",
    "            for i in range(start, end):\n",
    "                page = doc[i]\n",
    "                pix = page.get_pixmap(dpi=300)\n",
    "                img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "                text = pytesseract.image_to_string(img, lang=lang)\n",
    "\n",
    "                documents.append(\n",
    "                    Document(\n",
    "                        page_content=text.strip(),\n",
    "                        metadata={\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": i,\n",
    "                            \"total_pages\": total_pages,\n",
    "                            \"page_label\": str(i + 1)\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(\n",
    "                [{\"page_content\": d.page_content, \"metadata\": d.metadata} for d in documents],\n",
    "                f,\n",
    "                ensure_ascii=False,\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jKG:\n",
    "    def __init__(self, uri: str, user: str, pwd: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def get_lessons_for_topic(self, topic_name: str) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH (t:Topic {name: $topic_name})-[:HAS_LESSON]->(l:Lesson)\n",
    "        RETURN l.title AS title, l.start_page AS start_page, l.end_page AS end_page\n",
    "        ORDER BY l.title\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, topic_name=topic_name)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    def find_branch_for_topic(self, topic_name: str) -> Optional[str]:\n",
    "        query = \"\"\"\n",
    "        MATCH (b:Branch)-[:HAS_TOPIC]->(t:Topic {name: $topic_name})\n",
    "        RETURN b.name AS branch_name\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            rec = session.run(query, topic_name=topic_name).single()\n",
    "            return rec[\"branch_name\"] if rec else None\n",
    "\n",
    "    def list_all_topics(self) -> List[str]:\n",
    "        query = \"MATCH (t:Topic) RETURN t.name AS name ORDER BY t.name\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query)\n",
    "            return [record[\"name\"] for record in result]\n",
    "\n",
    "    def fetch_all_lesson_embeddings(self) -> List[Dict]:\n",
    "        cypher = \"\"\"\n",
    "        MATCH (t:Topic)-[:HAS_LESSON]->(l:Lesson)\n",
    "        WHERE l.vector_embedding IS NOT NULL\n",
    "        RETURN t.name AS topic, l.title AS lesson, l.vector_embedding AS embedding\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            records = session.run(cypher)\n",
    "            return [record.data() for record in records]\n",
    "\n",
    "    def fetch_lesson_images(self, lesson_title: str) -> List[Dict]:\n",
    "        cypher = \"\"\"\n",
    "        MATCH (l:Lesson {title: $title})-[:HAS_IMAGE]->(img:Image)\n",
    "        RETURN img.name AS name, img.caption AS caption, img.page AS page\n",
    "        ORDER BY img.page\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            return session.run(cypher, title=lesson_title).data()\n",
    "\n",
    "def render_pdf(mem: SessionMemory, outfile: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Renders a PDF report including summary, Q&A, quiz, and final feedback.\n",
    "    Handles images and styled text (like **bold**) for better layout.\n",
    "    \"\"\"\n",
    "    from reportlab.platypus import Paragraph\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.lib.enums import TA_RIGHT\n",
    "    from reportlab.lib import colors\n",
    "\n",
    "    c = pdf_canvas.Canvas(str(outfile), pagesize=A4)\n",
    "    w, h = A4\n",
    "    margin_top, margin_bottom = 40, 40\n",
    "    leading = 20\n",
    "    y = h - margin_top\n",
    "\n",
    "    # Styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    rtl_style = ParagraphStyle(\n",
    "        name=\"RTL\",\n",
    "        fontName=ARABIC_FONT_NAME,\n",
    "        fontSize=13,\n",
    "        leading=leading,\n",
    "        alignment=TA_RIGHT,\n",
    "        textColor=colors.black,\n",
    "    )\n",
    "    \n",
    "    def new_page():\n",
    "        nonlocal y\n",
    "        c.showPage()\n",
    "        y = h - margin_top\n",
    "\n",
    "    def wrap_line(line, width=80):\n",
    "        words, buf, out = line.split(), [], []\n",
    "        for w_ in words:\n",
    "            if sum(len(x) for x in buf) + len(w_) + len(buf) > width:\n",
    "                out.append(\" \".join(buf))\n",
    "                buf = [w_]\n",
    "            else:\n",
    "                buf.append(w_)\n",
    "        if buf:\n",
    "            out.append(\" \".join(buf))\n",
    "        return out\n",
    "\n",
    "    def draw_text(line: str, font=ARABIC_FONT_NAME, fsize=13):\n",
    "        nonlocal y\n",
    "        if y - leading < margin_bottom:\n",
    "            new_page()\n",
    "        # Check for markdown-style bold and set font weight\n",
    "        bold_parts = re.findall(r\"\\*\\*(.*?)\\*\\*\", line)\n",
    "        if bold_parts:\n",
    "            parts = re.split(r\"(\\*\\*.*?\\*\\*)\", line)\n",
    "            for part in parts:\n",
    "                if part.startswith(\"**\") and part.endswith(\"**\"):\n",
    "                    text = rtl(strip_unsupported(part[2:-2]))\n",
    "                    c.setFont(font + \"-Bold\", fsize)  # You need the bold variant installed\n",
    "                else:\n",
    "                    text = rtl(strip_unsupported(part))\n",
    "                    c.setFont(font, fsize)\n",
    "                c.drawRightString(w - margin_bottom, y, text)\n",
    "                y -= leading\n",
    "        else:\n",
    "            c.setFont(font, fsize)\n",
    "            c.drawRightString(w - margin_bottom, y, rtl(strip_unsupported(line)))\n",
    "            y -= leading\n",
    "\n",
    "\n",
    "    def draw_image(img_path: str, alt: str):\n",
    "        nonlocal y\n",
    "        if img_path.startswith(\"assets/book_images/\"):\n",
    "            img_path = img_path.replace(\"assets/book_images/\", \"\")\n",
    "        full_path = f\"{IMG_DIR}/{img_path}\".replace(\" \", \"\")\n",
    "\n",
    "        try:\n",
    "            im = Image.open(full_path)\n",
    "        except FileNotFoundError:\n",
    "            draw_text(f\"[ÿµŸàÿ±ÿ© ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ©] {alt}\")\n",
    "            return\n",
    "\n",
    "        iw, ih = im.size\n",
    "        scale = min(MAX_IMG_W / iw, MAX_IMG_H / ih, 1.0)\n",
    "        dw, dh = iw * scale, ih * scale\n",
    "\n",
    "        if y - dh - leading < margin_bottom:\n",
    "            new_page()\n",
    "\n",
    "        c.drawInlineImage(full_path, w - margin_bottom - dw, y - dh, width=dw, height=dh)\n",
    "        y -= dh + leading // 2\n",
    "        draw_text(alt, font=ARABIC_FONT_NAME, fsize=11)\n",
    "\n",
    "    def draw_rich_block(title: str, raw_md: str | list):\n",
    "        nonlocal y\n",
    "        draw_text(title, fsize=15)\n",
    "        c.setStrokeColorRGB(0.6, 0.6, 0.6)\n",
    "        c.line(margin_bottom, y + 6, w - margin_bottom, y + 6)\n",
    "        y -= leading // 2\n",
    "\n",
    "        if isinstance(raw_md, list):\n",
    "            lines = []\n",
    "            for slide in raw_md:\n",
    "                if isinstance(slide, dict):\n",
    "                    text = slide.get(\"text\", \"\")\n",
    "                    text = re.sub(r\"<b>(.*?)</b>\", r\"**\\1**\", text)\n",
    "                    image = slide.get(\"image\")\n",
    "                    lines.extend(text.splitlines())\n",
    "                    if image:\n",
    "                        lines.append(f\"![]({image})\")\n",
    "            raw_md = \"\\n\".join(lines)\n",
    "\n",
    "        for paragraph in raw_md.splitlines():\n",
    "            paragraph = paragraph.strip()\n",
    "            if not paragraph:\n",
    "                y -= leading // 2\n",
    "                continue\n",
    "\n",
    "            m = MD_IMG.fullmatch(paragraph)\n",
    "            if m:\n",
    "                alt, path = m.group(1).strip(), m.group(2).strip()\n",
    "                draw_image(path, alt)\n",
    "                continue\n",
    "\n",
    "            idx = 0\n",
    "            for m in MD_IMG.finditer(paragraph):\n",
    "                pre = paragraph[idx:m.start()].rstrip()\n",
    "                if pre:\n",
    "                    for l in wrap_line(pre):\n",
    "                        draw_text(l)\n",
    "                alt, path = m.group(1).strip(), m.group(2).strip()\n",
    "                draw_image(path, alt)\n",
    "                idx = m.end()\n",
    "            tail = paragraph[idx:].rstrip()\n",
    "            if tail:\n",
    "                for l in wrap_line(tail):\n",
    "                    draw_text(l)\n",
    "\n",
    "        y -= leading // 2\n",
    "\n",
    "    draw_text(\"üìó ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ÿπŸÑŸëŸÖ\", fsize=20)\n",
    "    y -= leading\n",
    "\n",
    "    if \"chapter_summary\" in mem:\n",
    "        for chapter in mem[\"chapter_summary\"]:\n",
    "            title = chapter.get(\"title\", \"ŸÖŸÑÿÆŸëÿµ ÿßŸÑÿØÿ±ÿ≥\")\n",
    "            slides = chapter.get(\"slides\", [])\n",
    "            draw_rich_block(title, slides)\n",
    "\n",
    "    if \"qa_history\" in mem:\n",
    "        lines = []\n",
    "        for q, a in mem[\"qa_history\"]:\n",
    "            lines.append(f\"‚ùì {q}\\nüì• {a}\\n\")\n",
    "        draw_rich_block(\"ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© Ÿà ÿßŸÑÿ£ÿ¨Ÿàÿ®ÿ©:\", \"\\n\".join(lines))\n",
    "\n",
    "    if \"quiz_log\" in mem:\n",
    "        q_lines = []\n",
    "        for idx, qd in enumerate(mem[\"quiz_log\"], 1):\n",
    "            q_text = qd.get(\"q\", \"\")\n",
    "            correct = qd.get(\"a\", \"?\")\n",
    "            q_lines.append(f\"{idx}) {q_text}\")\n",
    "            if qd[\"type\"] == \"mc\":\n",
    "                q_lines.append(\"   \" + \"ÿå \".join(qd[\"options\"]))\n",
    "            q_lines.append(f\"   ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©: {correct}\\n\")\n",
    "        draw_rich_block(\"ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±:\", \"\\n\".join(q_lines))\n",
    "\n",
    "    score_txt = \"\"\n",
    "    if \"feedback_note\" in mem:\n",
    "        score_txt += mem[\"feedback_note\"]\n",
    "    if score_txt:\n",
    "        draw_rich_block(\"ÿßŸÑÿ™ŸÇŸäŸäŸÖ ÿßŸÑŸÜŸáÿßÿ¶Ÿä:\", score_txt)\n",
    "\n",
    "    c.save()\n",
    "    return outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T13:36:29.800950Z",
     "iopub.status.busy": "2025-06-12T13:36:29.800684Z",
     "iopub.status.idle": "2025-06-12T13:36:29.804819Z",
     "shell.execute_reply": "2025-06-12T13:36:29.804001Z",
     "shell.execute_reply.started": "2025-06-12T13:36:29.800910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyCxEU-6lVV1PREeEy29pfDdFwmuKcTU7mc\"\n",
    "\n",
    "URI = \"neo4j+s://3e253ce0.databases.neo4j.io\"\n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"eMH4uA1k--yp1Ugwev9vXbXPnzVVo3QVaRLZ7Sh4_gU\"  # ‚Üê change to your Neo4j password\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Assistant (Arabic/Tunisian) \n",
    "- **OCR & PDF Retrieval:** Processes Arabic PDFs with semantic embedding and context-aware retrieval using HuggingFace models and Chroma.\n",
    "- **CrewAI Agents:** Specialized Tunisian-Arabic agents for summarization, Q&A, quizzes, and personalized feedback.\n",
    "- **FastAPI Integration:** RESTful endpoints for interactive lessons, quizzes, and automated PDF report generation tailored for students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T13:36:39.026894Z",
     "iopub.status.busy": "2025-06-12T13:36:39.026620Z",
     "iopub.status.idle": "2025-06-12T13:41:25.359889Z",
     "shell.execute_reply": "2025-06-12T13:41:25.359355Z",
     "shell.execute_reply.started": "2025-06-12T13:36:39.026871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 13:36:58.788624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749735419.036328     890 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749735419.113886     890 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/761k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/808 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached OCR data from /kaggle/working/-     .pdf.json\n",
      "Downloading ngrok ...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_890/1638984576.py:104: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  QA_MEMORY = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://a91c-34-90-35-83.ngrok-free.app\" -> \"http://localhost:8000\"       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [890]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"OPTIONS /summary HTTP/1.1\" 200 OK\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÿ•ŸÜÿ™Ÿä ŸÖÿπŸÑŸëŸÖ/ÿ© ÿ™ŸàŸÜÿ≥Ÿä/ÿ©ÿõ ŸáÿØŸÅŸÉ ÿ™ÿ®ÿ≥Ÿëÿ∑ ŸÖÿ≠Ÿàÿ± ‚ÄúÿßŸÑÿ™ŸÜŸÇŸÑ‚Äù ŸÖŸÜ ŸÅÿ±ÿπ ‚Äúÿ£ÿ≠Ÿäÿßÿ°‚Äù ŸÑÿ™ŸÑŸÖŸäÿ∞ ŸÅŸä\n",
      "ÿßŸÑÿ≥ŸÜÿ© ÿßŸÑÿ±ÿßÿ®ÿπÿ© ÿßÿ®ÿ™ŸÄÿØÿßÿ¶Ÿä. ÿ±ŸÉŸëÿ≤ ÿπŸÑŸâ ÿßŸÑŸÅŸáŸÖÿå ÿ±ÿ®ÿ∑ ÿßŸÑÿ£ŸÅŸÉÿßÿ± ÿ®ÿ≠Ÿäÿßÿ™Ÿà ÿßŸÑŸäŸàŸÖŸäÿ©ÿå\n",
      "Ÿàÿ™ŸÜŸàŸäÿπ ÿßŸÑÿ£ŸÖÿ´ŸÑÿ©.\n",
      "\n",
      "ÿßŸÑŸÖÿπÿ∑Ÿäÿßÿ™ ŸÇÿØÿßŸÖŸÉ:\n",
      "‚îå‚îÄ ÿßŸÑÿØÿ±Ÿàÿ≥ ÿßŸÑŸÅÿ±ÿπŸäŸëÿ©:\n",
      "‚Ä¢ ÿ£ŸÜŸÖÿßÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ ÿπŸÜÿØ ÿßŸÑÿ≠ŸäŸàÿßŸÜ\n",
      "‚Ä¢ ÿ™ŸÉŸäŸÅ ÿßŸÑÿπÿ∂Ÿà ŸÖÿπ ŸÜŸÖÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ\n",
      "\n",
      "‚îå‚îÄ ŸÖŸÇÿ™ÿ∑ŸÅÿßÿ™ ŸÖŸÜ ÿßŸÑŸÉÿ™ÿßÿ® (ÿ™ÿ≥ÿ™ÿπŸÖŸÑŸáÿß ŸÉÿßŸÜ ÿ™ÿ≠ÿ® ÿ™ŸÇÿ™ÿ®ÿ≥ ÿ¨ŸÖŸÑÿ© ŸàŸÑÿß ÿ™Ÿàÿ∂Ÿäÿ≠):\n",
      "ÿßŸÑÿ™ŸÜŸÇŸÑ ÿπŸÜÿØ ÿß ŸÑÿ≠ŸäŸàÿßŸÜ ‚Äè ÿ®\n",
      "\n",
      "ÿßŸÑŸÖŸàÿ∂Ÿàÿπ : ÿ™ŸÉŸäŸÅ ÿßŸÑÿπÿ∂Ÿà ŸÖÿ≠ ŸÜŸäÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ\n",
      "\n",
      "ÿßŸÑŸáÿØŸÅ : ÿ£ÿ™Ÿêÿ®ŸéŸäŸéŸÜŸí ÿ™ŸÉŸäŸÇ ÿßŸÑÿπÿ∂Ÿà ŸÖÿπ ŸÜŸÖÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ\n",
      "\n",
      "¬©\n",
      "- ÿ£ÿ±ÿ≥ŸÖ ÿßŸÑÿ∑ÿ∞ÿ±ŸÅ ÿßŸÑÿ£ŸÖÿßŸÖŸä ŸàÿßŸÑÿÆŸÑŸÅŸä ŸÑÿ£ÿ±ŸÜÿ® ÿ´ŸÖ ÿ£ŸÇÿßÿ±ŸÜ ÿ®ŸäŸÜŸáŸÖÿß\n",
      "- ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜ ÿ∑ŸàŸÑ ŸÇŸÅÿ≤ÿ© ÿ≠ŸäŸàÿßŸÜ Ÿäÿ™ŸÜŸÇŸÑ ŸÇŸÅÿ≤ÿß ÿ´ŸÖ ÿ£ŸÇÿßÿ±ŸÜ ÿ®ŸäŸÜ ÿ∑ŸàŸÑ ŸÇŸÅÿ≤ÿ© ÿßŸÑÿ≠ŸäŸàÿßŸÜŸàÿ∑ŸàŸÑ ÿ¨ÿ≥ŸÖŸá\n",
      "- ÿ£ÿ≥ŸÖŸä ÿßŸÑÿ£ÿπÿ∂ÿßÿ° ÿßŸÑÿ™Ÿä ÿ™ŸÖŸÉ ÿßŸÑÿ≥ŸÖŸÉÿ© ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿßŸÅÿ∏ÿ© ÿπŸÑŸâ ÿ™Ÿàÿßÿ≤ŸÜŸáÿß ÿ£ŸÇŸÜŸéÿßÿ° ÿßŸÑÿ≥ÿ®ÿßÿ≠ÿ©\n",
      "- ÿ£ÿ±ÿ≥ŸÖ ÿπŸÑŸâ ŸÉÿ±ÿßÿ≥Ÿä ÿ¨ŸÜÿßÿ≠Ÿä ÿ∑ÿßÿ¶ÿ± ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑÿ•ŸÇŸÑÿßÿπ . ÿßŸÑÿ™ÿ≠ŸÑŸäŸÇ ¬ª ÿßŸÑÿ™ÿ≤ŸàŸÑ\n",
      "- ÿ£ŸÅÿ≥ÿ± ŸÑŸÖÿßÿ∞ÿß : Ÿäÿ£ÿÆÿ∞ ÿßŸÑÿ¨ŸÜÿßÿ≠ÿßŸÜ ÿ¥ŸÉŸÑÿß ŸÖÿπŸäŸÜÿß ÿÆŸÑÿßŸÑ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ±ÿßÿ≠ŸÑ.\n",
      "- ÿ™ÿ™ŸÖŸäÿ±Ÿå ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ™ŸÜŸÇŸÑ ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿßŸÑÿπÿØŸà ÿ®ÿ¢ÿ™ÿ≥ÿßÿπ ÿßŸÑŸÇŸÅÿµ ÿßŸÑÿµÿØÿ±Ÿä ŸàŸÇŸàÿ© ÿπÿ∂ŸÑÿßÿ™Ÿáÿß\n",
      "ÿ•ŸÑŸâ ÿ¨ÿßŸÜÿ® ÿ∑ŸàŸÑ ÿßŸÑŸÇŸàÿßÿ¶ŸÖ ŸàÿßŸÜÿ™ÿµÿßÿ®Ÿáÿß\n",
      "\n",
      "- ŸäŸÅŸàŸÇ ÿ∑ŸàŸÑ ŸÇŸÅÿ≤ÿ© ÿßŸÑÿ£ÿ±ŸÜÿ® 6 ŸÖÿ±ÿßÿ™ ÿ∑ŸàŸÑ ÿ¨ÿ≥ŸÖŸáÿß\n",
      "\n",
      "- ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑÿ≥ÿ®ÿßÿ≠ÿ© ŸäÿØŸÅÿπ ÿ™ÿ≠ÿ±ŸÉ ÿßŸÑÿ∞ŸÜÿ® ÿßŸÑÿ≥ŸÖŸÉÿ© ÿ•ŸÑŸâ ÿßŸÑÿ£ŸÖÿßŸÖ ÿ®ŸäŸÜŸÖÿß ÿ™ÿ≥ÿßÿπÿØ ÿ®ŸÇŸäÿ© ÿßŸÑÿ≤ÿπÿßŸÜŸÅ ÿπŸÑŸâ\n",
      "\n",
      "ÿ™Ÿàÿßÿ≤ŸÜ ÿßŸÑÿ≥ŸÖŸÉÿ© ŸàÿßŸÑÿ™ÿ≠ŸÉŸÖ ŸÅŸä ÿ™ÿ≠ÿ±ŸÉŸáÿß\n",
      "\n",
      "7 ÿ™ÿπŸÑŸÖÿ™ :\n",
      "\n",
      "ÿ£ÿ™ÿ£ŸÖŸÑ ÿßŸÑÿ¨ÿØŸàŸÑ Ÿàÿ£ÿ≥ÿ™ÿπŸäŸÜ ÿ®Ÿá ŸÑŸÉÿ™ÿßÿ®ÿ© ÿ£ŸáŸÖ ŸÖÿß ÿ™ŸàÿµŸÑÿ™ ÿ•ŸÑŸäŸá ŸÅŸä ŸÜŸáÿßŸäÿ© ÿßŸÑÿØŸëÿ±ÿ≥ (ÿπŸÑŸâ ŸÉÿ±ÿßÿ≥Ÿä)\n",
      "\n",
      "  \n",
      "\n",
      "ÿÆÿß 2\n",
      "ÿßŸÑŸÖŸàÿ∂Ÿàÿπ : ÿ£ŸÜŸäÿßÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ ÿπŸÜÿ± ÿßŸÑÿ≠ŸäŸàÿßÿ™ (ÿ®ÿ±ÿß - ÿ®ÿ≠ÿ±ÿß - ÿ¨Ÿàÿß)\n",
      "ÿßŸÑŸáÿØŸÅ : ÿ£ÿ™Ÿéÿ®ŸéŸäŸÜ ÿ£ÿ™ŸÖÿßÿ∑ ÿ™ÿ™ŸÇŸÑ ÿ™ŸÇŸÑ ÿßŸÑÿ≠ŸäŸà ŸÜÿßÿ™\n",
      "\n",
      "¬© -ÿ®. -ÿßÿ®ÿ≠ÿ´ ŸÅŸä ŸÖŸàÿ≥Ÿàÿπÿ™ŸÉ ÿßŸÑÿπŸÑŸÖŸäÿ© ÿ£Ÿà ÿ£ÿ®ÿ≠ÿ± ŸÅŸä ÿπÿßŸÑŸÖ ÿßŸÑÿ£ŸÜÿ™ÿ±ŸÜÿßÿ™ ŸÑŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿ£ÿ≥ŸÖÿßÿ° ÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ®ÿ±Ÿäÿ©\n",
      "Ÿàÿ®ÿ≠ÿ±Ÿäÿ© Ÿàÿ∑ŸäŸàÿ± ŸÜÿßÿØÿ±ÿ© Ÿàÿßÿ∞ŸÉÿ± ÿ£ŸÜŸÖÿßÿ∑ ÿ™ŸÜŸÇŸÑŸáÿß ŸàÿßŸÑÿ£Ÿàÿ≥ÿßÿ∑ ÿßŸÑÿ™Ÿä ÿ™ÿ™ŸÜŸÇŸÑ ŸÅŸäŸáÿß\n",
      "\n",
      " \n",
      "\n",
      "ŸáŸá\n",
      "\n",
      "9 ŸÉŸá\n",
      "\n",
      "ÿ™ÿ£ŸÖŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿµŸàÿ±ÿ©\n",
      "ŸÉŸäŸÅ Ÿäÿ™ŸÖ ÿ™ŸÜŸÇŸÑ ÿßŸÑÿ≠ŸÖÿßŸÖÿ© Ÿà ÿßŸÑÿ®ÿ∑ÿ© ÿü\n",
      "\n",
      "ŸÑÿß ÿßŸÖ\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "6\n",
      "ÿßŸÑÿ™ŸÜŸÇŸÑ ÿπŸÜÿØ ÿß ŸÑÿ≠ŸäŸàÿßŸÜ ‚Äè ÿ®\n",
      "\n",
      "ÿßŸÑŸÖŸàÿ∂Ÿàÿπ : ÿ™ŸÉŸäŸÅ ÿßŸÑÿπÿ∂Ÿà ŸÖÿ≠ ŸÜŸäÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ\n",
      "\n",
      "ÿßŸÑŸáÿØŸÅ : ÿ£ÿ™Ÿêÿ®ŸéŸäŸéŸÜŸí ÿ™ŸÉŸäŸÇ ÿßŸÑÿπÿ∂Ÿà ŸÖÿπ ŸÜŸÖÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ\n",
      "\n",
      "¬©\n",
      "- ÿ£ÿ±ÿ≥ŸÖ ÿßŸÑÿ∑ÿ∞ÿ±ŸÅ ÿßŸÑÿ£ŸÖÿßŸÖŸä ŸàÿßŸÑÿÆŸÑŸÅŸä ŸÑÿ£ÿ±ŸÜÿ® ÿ´ŸÖ ÿ£ŸÇÿßÿ±ŸÜ ÿ®ŸäŸÜŸáŸÖÿß\n",
      "- ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜ ÿ∑ŸàŸÑ ŸÇŸÅÿ≤ÿ© ÿ≠ŸäŸàÿßŸÜ Ÿäÿ™ŸÜŸÇŸÑ ŸÇŸÅÿ≤ÿß ÿ´ŸÖ ÿ£ŸÇÿßÿ±ŸÜ ÿ®ŸäŸÜ ÿ∑ŸàŸÑ ŸÇŸÅÿ≤ÿ© ÿßŸÑÿ≠ŸäŸàÿßŸÜŸàÿ∑ŸàŸÑ ÿ¨ÿ≥ŸÖŸá\n",
      "- ÿ£ÿ≥ŸÖŸä ÿßŸÑÿ£ÿπÿ∂ÿßÿ° ÿßŸÑÿ™Ÿä ÿ™ŸÖŸÉ ÿßŸÑÿ≥ŸÖŸÉÿ© ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿßŸÅÿ∏ÿ© ÿπŸÑŸâ ÿ™Ÿàÿßÿ≤ŸÜŸáÿß ÿ£ŸÇŸÜŸéÿßÿ° ÿßŸÑÿ≥ÿ®ÿßÿ≠ÿ©\n",
      "- ÿ£ÿ±ÿ≥ŸÖ ÿπŸÑŸâ ŸÉÿ±ÿßÿ≥Ÿä ÿ¨ŸÜÿßÿ≠Ÿä ÿ∑ÿßÿ¶ÿ± ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑÿ•ŸÇŸÑÿßÿπ . ÿßŸÑÿ™ÿ≠ŸÑŸäŸÇ ¬ª ÿßŸÑÿ™ÿ≤ŸàŸÑ\n",
      "- ÿ£ŸÅÿ≥ÿ± ŸÑŸÖÿßÿ∞ÿß : Ÿäÿ£ÿÆÿ∞ ÿßŸÑÿ¨ŸÜÿßÿ≠ÿßŸÜ ÿ¥ŸÉŸÑÿß ŸÖÿπŸäŸÜÿß ÿÆŸÑÿßŸÑ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ±ÿßÿ≠ŸÑ.\n",
      "ŸÉŸÉÿßŸâ ÿßŸÑŸÉŸÑŸÖŸäÿ©\n",
      "ŸÑÿß -ÿ£ÿ≠ŸäŸàÿßÿ°\n",
      "-ÿßŸÑÿ≠Ÿàÿßÿ≥ ŸàÿßŸÑŸàŸÇÿßŸäÿ© ŸÖŸÜ ÿßŸÑÿ£ŸÖÿ±ÿßÿ∂\n",
      "\n",
      "-ÿßŸÑÿ™ŸÜŸÇŸÑ\n",
      "\n",
      "-ÿßŸÑÿ™ÿ∫ÿ∞Ÿäÿ©\n",
      "-ÿßŸÑÿ™ŸÉÿßÿ´ÿ± ŸàÿßŸÑŸÜŸÖŸà\n",
      "-ÿßŸÑÿ™ŸÜŸÅÿ≥\n",
      "ÿßŸÑÿØÿ±ÿ≥\n",
      "- ÿßŸÑÿ≠Ÿàÿßÿ≥ Ÿàÿ£ÿπÿ∂ÿßÿ° ÿßŸÑÿ≠ÿ≥' 0ÿ∫\n",
      "Ÿàÿ∏ÿßÿ¶ŸÅ ÿßŸÑÿ¨ŸÑÿØ ŸàŸàŸÇÿßŸäÿ™Ÿá ŸáŸÖŸá ŸÖŸáŸÖŸá ÿπ ŸÖŸÖ 9\n",
      "ÿßŸÑÿ™ÿ£ÿ´Ÿäÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÑÿ®Ÿäÿ© ÿπŸÑŸâ ÿ≠ÿßÿ≥ÿ™Ÿä ÿßŸÑÿ≥ŸÖÿπ ŸàÿßŸÑÿßÿ®ÿµÿßÿ± ŸÖŸÖŸÖ 120\n",
      "ÿ≠ŸÖÿßŸäÿ© ÿßŸÑÿ≥ŸÖÿπ ŸàÿßŸÑÿßÿ®ÿµÿßÿ± ŸÖŸÜ ÿßŸÑŸÖÿ§ÿ´ÿ±ÿßÿ™ ÿßŸÑŸÖÿ≤ÿπÿ¨ÿ© 00\n",
      "ÿ™ÿ£ÿ´Ÿäÿ± ŸÖÿ±ÿ∂ ÿßŸÑÿ≤ŸÉÿßŸÖ ÿπŸÑŸä ÿßŸÑÿ¨ÿ≥ŸÖ 0\n",
      "ÿ≠ÿµÿ© ÿ•ÿØŸÖÿßÿ¨ ŸÖŸÖŸÖ ŸÖŸá ŸÖŸÖŸÖ 2200000\n",
      "ÿ≠ÿµÿ© ÿØÿπŸÖ 211100\n",
      "ÿ£ŸÜŸÖÿßÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ ÿπŸÜÿØ ÿßŸÑÿ≠ŸäŸàÿßŸÜ 2\n",
      "ÿ™ŸÉŸäŸÅ ÿßŸÑÿπÿ∂Ÿà ŸÖÿπ ŸÜŸÖÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ 000\n",
      "ÿ≠ÿµÿ© ÿ™ŸÇŸäŸäŸÖ 00\n",
      "- ŸÖÿµÿßÿØÿ± ÿßŸÑÿ£ÿ∫ÿ∞Ÿäÿ© ŸÖŸÖÿπ ŸÖŸÖÿπ ŸÖÿπ ÿπŸÖ ÿπŸÖ ŸÑ 3660\n",
      "- ŸÖÿ≥ÿßÿ± ÿßŸÑÿ£ÿ∫ÿ∞Ÿäÿ© Ÿàÿ™ÿ≠ŸàŸÑŸáÿß ÿØÿßÿÆŸÑ ÿßŸÑÿ£ŸÜŸäŸàÿ® ŸÑÿ≠ŸäŸàÿßŸÜ ÿπÿßÿ¥ÿ® 000 41\n",
      "- ÿ£ŸÜŸàÿßÿπ ÿßŸÑÿ£ÿ≥ŸÜÿßŸÜ ŸàŸàÿ∏ÿßÿ¶ŸÅŸáÿß 0\n",
      "- ŸàŸÇÿßŸäÿ© ÿßŸÑÿ£ÿ≥ŸÜÿßŸÜ 1ÿ∫\n",
      "- ÿ≠ÿµÿ© ÿ•ÿØŸÖÿßÿ¨ ŸÖŸÖŸÖ 52\n",
      "- ÿ≠ÿµÿ© ÿ™ŸÇŸäŸäŸÖ ÿπ ÿπ ŸÖ ÿπ ŸÖŸÖ ŸÖ ŸÖ 54\n",
      "- ÿßŸÑÿ™ŸÉÿßÿ´ÿ± ÿØŸàŸÜ Ÿäÿ∞Ÿàÿ± 1010101111100\n",
      "- ÿ≠ÿµÿ© ÿ™ŸÇŸäŸäŸÖ 101100\n",
      "- ÿ£ÿπÿ∂ÿßÿ° ÿßŸÑÿ™ŸÜŸÅÿ≥ ŸÑÿØŸâ ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ŸÖŸÖ ŸÖŸÖ ŸÖŸÖ ŸÖ 6200\n",
      "- ÿ£ÿπÿ∂ÿßÿ° ÿßŸÑÿ™ŸÜŸÅÿ≥ ŸÑÿØŸâ ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ : ÿßŸÑÿ±ÿ¶ÿ™ÿßŸÜ ÿπŸÜÿØ ÿßŸÑÿÆÿ±ŸàŸÅ 66\n",
      "- ÿ£ÿπÿ∂ÿßÿ° ÿßŸÑÿ™ŸÜŸÅÿ≥ ŸÑÿØŸâ ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ : ÿßŸÑÿ∫ŸÑÿßÿµŸÖ ÿπŸÜÿØ ÿßŸÑÿ≥ŸÖŸÉÿ© 0\n",
      "- ÿ≠ÿµÿ© ÿ•ÿØŸÖÿßÿ¨ 0ÿ∫\n",
      "\n",
      "- ÿ≠ÿµÿ© ÿ™ŸÇŸäŸäŸÖ ŸÑ 0000|\n",
      "ÿßŸÑÿ≥ÿ™ÿØ :\n",
      "ŸÅÿ™ÿ≠ÿ™ ÿßŸÑÿ™ŸÑŸÅÿßÿ≤ Ÿàÿ¨ŸÑÿ≥ÿ™ ÿ£ÿ™ŸÅÿ±Ÿäÿ¨ ÿπŸÑŸâ ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑÿµŸàÿ± ÿßŸÑŸÖÿ™ÿ≠ÿ±ŸÉÿ© ŸÅŸÉÿßŸÜ ÿ£ÿ®ÿ∑ÿßŸÑ ÿßŸÑÿ≠ŸÑŸÇÿ© ÿ≠ŸäŸàÿßŸÜÿßÿ™. ŸÅŸä\n",
      "ŸÜŸáÿßŸäÿ© ÿßŸÑÿ®ÿ±ŸÜÿßŸÖÿ¨ Ÿàÿ¨ÿØÿ™ŸÜŸä ÿ£ÿ∑ÿ±ÿ≠ ÿπŸÑŸâ ŸÜŸÅÿ≥Ÿä ÿπÿØÿ© ÿ™ÿ≥ÿßÿ§ŸÑÿßÿ™ ŸÅŸáŸÑÿßŸã ÿ≥ÿßÿπÿØÿ™ŸÜŸä ÿπŸÑŸâ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÜŸáÿß\n",
      "\n",
      "  \n",
      "\n",
      "ÿ£ÿ™ÿ£ŸÖŸÑ ÿßŸÑŸÖÿÆÿ∑ÿ∑ Ÿàÿ£ÿ¥ÿ∑ÿ® ÿßŸÑÿπŸÜÿµÿ± ÿßŸÑÿ≤ÿßÿ¶ÿØ\n",
      "\n",
      "ÿÆÿµÿßÿ¶ÿµ ÿßŸÑÿ£ÿπÿ∂ÿßÿ° ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ÿπŸÑŸâ ÿßŸÑÿ™ŸÜŸÇŸÑ ÿ®ÿ±ÿ£\n",
      "\n",
      "ÿ£ŸÇÿ±ÿ£ ÿ´ŸÖ ÿ£ÿπŸÑŸÑ :\n",
      "ÿ™ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿ≥ŸÑŸëÿ≠ŸÅÿßÿ© ÿßŸÑÿ®ÿ≠ÿ±Ÿäÿ© ÿßŸÑÿ™ŸÜŸÇŸÑ ŸÅŸä ÿßŸÑÿ®Ÿëÿ± ŸÑŸÖÿßÿ∞ÿß ÿü\n",
      "ÿ™ÿ™ŸÖŸäÿ≤ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ™ŸÜŸÇŸÑ ŸÇŸÅÿ≤ÿß ÿ®ÿ∑ŸàŸÑ ÿßŸÑŸÇŸàÿßÿ¶ŸÖ ÿßŸÑÿÆŸÑŸÅŸäÿ© ŸàŸÇÿµÿ± ÿßŸÑŸÇŸàÿßÿ¶ŸÖ ÿßŸÑÿ£ŸÖÿßŸÖŸäÿ©\n",
      "\n",
      "ÿ£ÿµŸÑÿ≠ ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ÿßŸÑÿÆÿßÿ∑ÿ¶ÿ© :\n",
      "- ŸÑÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ£ÿπÿ∂ÿßÿ° ÿ™ÿ≥ÿßÿπÿØŸáÿß ÿπŸÑŸâ ÿßŸÑÿ™ŸÉŸäŸÅ ŸÖÿπ ŸÜŸÖÿ∑ Ÿàÿßÿ≠ÿØ ŸÖŸÜ ÿßŸÑÿ™ŸÜŸÇŸÑ ŸÅŸÇÿ∑\n",
      "- ŸÑŸÉŸÑ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ£ÿπÿ∂ÿßÿ° ÿ™ÿ≥ÿßÿπÿØŸáÿß ÿπŸÑŸâ ÿßŸÑÿ™ŸÉŸäŸÅ ŸÖÿπ ŸÜŸÖÿ∑ŸäŸÜ ŸÖÿÆÿ™ŸÑŸÅŸäŸÜ ŸÖŸÜ ÿßŸÑÿ™ŸÜŸÇŸÑ\n",
      "- ÿ™ÿ™ŸÜŸÇŸÑ ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿßŸÑŸÖÿßÿ¶Ÿäÿ© ŸÇÿ±Ÿäÿ®ÿß ŸÖŸÜ ÿ≥ÿ∑ÿ≠ ÿßŸÑŸÖÿßÿ°.\n",
      "\n",
      "‚îå‚îÄ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ™ÿµÿßŸàÿ± ŸÖÿ±ÿ™ÿ®ÿ∑ÿ© (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä ÿ™ÿ≥ÿ™ÿπŸÖŸÑ ÿ®ÿπÿ∂Ÿáÿß):\n",
      "ÿØÿ±ÿ≥ ¬´ÿ£ŸÜŸÖÿßÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ ÿπŸÜÿØ ÿßŸÑÿ≠ŸäŸàÿßŸÜ¬ª ‚Äì ÿßŸÑÿ™ÿµÿßŸàÿ±:\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿ®ÿ∑ÿ™ŸäŸÜ ÿπŸÑŸâ ŸÜŸáÿ±.](page_26_img_23.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿ≠ŸÖÿßŸÖÿ© ÿ∞ÿßÿ™ ÿ£ŸÑŸàÿßŸÜ ŸÖÿ™ÿπÿØÿØÿ©.](page_26_img_24.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿ≥ŸéŸÖŸéŸÉŸé ŸÖŸèŸÑŸéŸàŸëŸÜ.](page_27_img_31.jpeg)\n",
      "* [ÿ∑ÿßÿ¶ÿ± ÿ£ÿ®Ÿäÿ∂ ÿ±ŸÖÿßÿØŸä Ÿäÿ∑Ÿäÿ± ŸÅŸàŸÇ ŸÖÿßÿ° ÿ£ÿ≤ÿ±ŸÇ.](page_27_img_32.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ÿ™ŸÖÿ´ŸÑ ÿ™ŸÖÿ≥ÿßÿ≠Ÿãÿß ÿ£ÿÆÿ∂ÿ±.](page_27_img_25.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿ∑ÿßÿ¶ÿ± ÿßŸÑÿ®ÿ∑.](page_27_img_26.jpeg)\n",
      "* [ŸÜŸÖŸàÿ∞ÿ¨Ÿå ÿ´ŸÑÿßÿ´Ÿä ÿßŸÑÿ£ÿ®ÿπÿßÿØ ŸÑŸÜŸÖÿ± ŸÖŸèÿµŸàŸëÿ±.](page_27_img_29.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ÿØÿ¨ÿßÿ¨ÿ© ÿ≠ŸÖÿ±ÿßÿ°.](page_27_img_28.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿ∑ÿßÿ¶ÿ± ŸÜÿπÿßŸÖÿ© ŸÅŸä Ÿàÿ∂ÿπŸäÿ© ÿ≥Ÿäÿ±.](page_27_img_27.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿ∑ÿßÿ¶ÿ± ÿ®ŸÜŸä ÿßŸÑŸÑŸàŸÜ ÿπŸÑŸâ ÿ∫ÿµŸÜ.](page_27_img_30.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑŸÄ ŸÉŸàÿßŸÑÿ© ÿ®ÿ±ÿ™ŸÇÿßŸÑŸäÿ© ÿßŸÑŸÑŸàŸÜ.](page_27_img_33.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿµŸÇÿ± ÿ®ÿßŸÑÿ¨ŸÜÿßÿ≠ŸäŸÜ ŸÖŸÅÿ™Ÿàÿ≠ŸäŸÜ.](page_29_img_34.jpeg)\n",
      "* [ÿ£ÿ±ŸÜÿ® ÿ®ÿ±ŸëŸä ŸäŸÇŸÅÿ≤ ÿ®ÿ≥ÿ±ÿπÿ©.](page_29_img_35.jpeg)\n",
      "\n",
      "ÿØÿ±ÿ≥ ¬´ÿ™ŸÉŸäŸÅ ÿßŸÑÿπÿ∂Ÿà ŸÖÿπ ŸÜŸÖÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ¬ª ‚Äì ÿßŸÑÿ™ÿµÿßŸàÿ±:\n",
      "* [ÿµŸàÿ±ÿ© ÿ≠ÿµÿßŸÜ ÿ®ÿ±ÿ™ŸÇÿßŸÑŸä ÿßŸÑŸÑŸàŸÜ ŸäŸÖÿ¥Ÿä.](page_31_img_37.jpeg)\n",
      "* [ÿ£ÿ±ŸÜÿ®Ÿå Ÿäÿ±ŸÉÿ∂Ÿè ÿ®ÿ≥ÿ±ÿπÿ©Ÿç.](page_31_img_38.jpeg)\n",
      "* [ÿµŸàÿ±ÿ©Ÿå ŸÑŸÜŸèÿ≥ÿ±Ÿç ÿ¨ÿßÿ±ÿ≠Ÿç ŸÖŸèÿ±ÿ≥ŸÖŸç.](page_31_img_36.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑÿ∑ÿßÿ¶ÿ± ÿ®ÿ∑ÿ© ŸÖŸÑŸàŸÜÿ©.](page_32_img_41.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ŸÑŸÜÿ≥ÿ± ÿ¨ŸÜÿßÿ≠ŸäŸá ŸÖŸÅÿ™Ÿàÿ≠ÿ©.](page_32_img_43.jpeg)\n",
      "* [ÿ≥ÿßŸÇÿß ÿ≠ÿµÿßŸÜ ŸÅŸä Ÿàÿ∂ÿπŸäÿ© ÿßŸÑŸÖÿ¥Ÿä.](page_32_img_42.jpeg)\n",
      "* [ÿ≥ŸÖŸÉÿ© ÿ®ÿ±ŸëÿßŸÇÿ© ÿ∞ÿßÿ™ ÿ£ŸÑŸàÿßŸÜ ŸÖÿßÿ¶Ÿäÿ©.](page_32_img_40.jpeg)\n",
      "* [ÿ≥ŸÖŸÉÿ© ÿµŸÅÿ±ÿßÿ° ÿ∞ÿßÿ™ ÿ≤ÿπÿßŸÜŸÅ ÿ≠ŸÖÿ±ÿßÿ°.](page_32_img_39.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ÿ™Ÿàÿ∂Ÿäÿ≠Ÿäÿ© ŸÑÿπÿ∂ŸÑÿßÿ™ ÿßŸÑŸÉŸàÿπ.](page_33_img_44.jpeg)\n",
      "* [ÿµŸàÿ±ÿ© ÿ™Ÿàÿ∂Ÿäÿ≠Ÿäÿ© ŸÑÿπÿ∂ŸÑÿßÿ™ ÿßŸÑÿ≥ÿßŸÇ.](page_33_img_45.jpeg)\n",
      "\n",
      "\n",
      "ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿπŸÖŸÑ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©:\n",
      "1) ÿ•ŸÅÿ™ÿ™ÿßÿ≠ŸäŸëÿ© ÿµÿ∫Ÿäÿ±ÿ© ÿ®ÿßŸÑÿØÿßÿ±ÿ¨ÿ© (ÿ≥ÿ∑ÿ±ŸäŸÜ ÿ•ŸÑŸâ Ÿ£ ÿ≥ÿ∑Ÿàÿ±) ÿ™ÿπÿ±ŸëŸÅ ŸÅŸäŸáÿß ÿ®ÿßŸÑŸÖÿ≠Ÿàÿ±\n",
      "   ŸàŸÑŸÖÿßÿ∞ÿß ŸäŸáŸÖŸë ÿßŸÑÿ™ŸÑŸÖŸäÿ∞ ŸÅŸä ÿ≠Ÿäÿßÿ™Ÿà.\n",
      "2)  ÿ®ÿπÿØ ÿßŸÑÿ•ŸÅÿ™ÿ™ÿßÿ≠Ÿäÿ©ÿå ÿßÿπŸÖŸÑ ŸÑŸÉŸÑ ÿØÿ±ÿ≥ ŸÅÿ±ÿπŸä ÿßŸÑŸä ÿπŸÜÿØŸÉ :\n",
      "    ‚Ä¢ ÿßÿ¥ÿ±ÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ÿ®ÿπÿ®ÿßÿ±ÿ© ŸÖÿ®ÿ≥Ÿëÿ∑ÿ©.\n",
      "    ‚Ä¢ ÿ£ÿπÿ∑ ŸÖÿ´ÿßŸÑŸãÿß ŸàÿßŸÇÿπŸäŸãÿß ŸÖŸÜ ÿ≠Ÿäÿßÿ© ÿßŸÑÿ∑ŸÅŸÑ (ÿßŸÑÿØÿßÿ±ÿå ÿßŸÑÿ≠ŸàŸÖÿ©ÿå ÿßŸÑÿ∑ÿ®Ÿäÿπÿ©‚Ä¶).\n",
      "    ‚Ä¢ ÿ•ÿ∞ÿß ÿπŸÜÿØŸÉ ÿµŸàÿ±ÿ© ÿ™Ÿàÿ∂Ÿëÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ©ÿå ÿ£ÿØÿ±ÿ¨Ÿáÿß ŸÅŸä ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿ¥ÿ±ÿßÿ¶ÿ≠ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ¥ŸÉŸÑ:\n",
      "      ![ŸàÿµŸÅ ŸÖÿÆÿ™ÿµÿ±](assets/book_images/page_6_img_0.jpeg)\n",
      "        ‚Ü≥ ÿßŸÑÿµŸàÿ± ÿßÿÆÿ™Ÿäÿßÿ±Ÿäÿ©ÿå ŸàŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ Ÿ£ ÿµŸàÿ± ŸÅŸä ŸÉÿßŸÖŸÑ ÿßŸÑŸÖŸÑÿÆÿµ.\n",
      "3) ÿ£ÿÆÿ™ŸÖ ÿ®ÿ≥ÿ∑ÿ± ŸäŸèŸÑÿÆŸëÿµ ÿßŸÑÿØÿ±ÿ≥ ŸàŸÇŸÑŸàÿß ÿßÿ∞ÿß ÿπŸÜÿØŸÉ ÿ≥ÿ§ÿßŸÑ ÿßŸÜŸÉ ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿÆÿßŸÜÿ© ÿßÿ≥ÿ¶ŸÑŸÜŸä.\n",
      "\n",
      "ÿ™ŸÜÿ®ŸäŸáÿßÿ™ ÿ£ÿ≥ŸÑŸàÿ®ŸäŸëÿ©:\n",
      "- ÿßŸÉÿ™ÿ® ÿ®ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ© ÿßŸÑÿÆŸÅŸäŸÅÿ©ÿå ÿ¨ŸèŸÖŸéŸÑ ŸÇÿµŸäÿ±ÿ©ÿå ŸÖŸÅÿ±ÿØÿßÿ™ ŸÖÿ£ŸÑŸàŸÅÿ©.\n",
      "- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ£ŸÅÿπÿßŸÑ ÿ£ŸÖÿ± ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©: ¬´ÿ¨ÿ±Ÿëÿ®¬ªÿå ¬´ÿ±ŸÉŸëÿ≤¬ªÿå ¬´ŸÑÿßÿ≠ÿ∏¬ª.\n",
      "- ŸÑÿß ÿ™ÿ∞ŸÉÿ± ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑÿµŸÅÿ≠ÿßÿ™ ŸàŸÑÿß ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿØÿßÿÆŸÑ ÿßŸÑŸÜÿµ (ÿ•ŸÑÿß ŸÅŸä ÿµŸäÿ∫ÿ© ÿßŸÑŸÖÿßÿ±ŸÉÿØÿßŸàŸÜ ![alt](path)).\n",
      "\n",
      "‚¨áÔ∏è **ÿßŸÑŸÖÿÆÿ±ÿ¨ÿßÿ™ Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ JSON ŸÅŸÇÿ∑ÿå ŸÖÿ∑ÿßÿ®ŸÇŸãÿß ŸÑŸáÿ∞ÿß ÿßŸÑŸáŸäŸÉŸÑ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑** ‚¨áÔ∏è\n",
      "{\n",
      "  \"title\": \"ÿØÿ±ÿ≥ ÿπŸÜ ÿßŸÑÿ™ŸÜŸÇŸÑ\",\n",
      "  \"slides\": [\n",
      "    { \"number\": \"1\", \"text\": \"ÿ¥ÿ±ÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ£ŸàŸÑŸâ ŸáŸÜÿß\", \"image\": \"assets/book_images/page_6_img_0.jpeg\" },\n",
      "    { \"number\": \"2\", \"text\": \"ÿ¥ÿ±ÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ© ŸáŸÜÿß\"},\n",
      "    { \"number\": \"3\", \"text\": \"‚Ä¶\", \"image\": \"assets/book_images/page_6_img_2.jpeg\" },\n",
      "    //  ÿ£ÿ∂ŸÅ ÿ¥ÿ±ÿßÿ¶ÿ≠ ÿ£ÿÆÿ±Ÿâ ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ®ŸÜŸäÿ©ÿõ ÿ•ÿ∞ÿß ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿµŸàÿ±ÿ©ÿå ŸÑÿß ÿ™ÿ∂ŸÅ ÿ≠ŸÇŸÑ ÿßŸÑÿµŸàÿ±ÿ© ŸÉÿßŸÑŸÖÿ´ÿßŸÑ ÿ±ŸÇŸÖ ÿßÿ´ŸÜÿßŸÜ\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /summary HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"OPTIONS /quiz HTTP/1.1\" 200 OK\n",
      "{'subject': 'ÿ£ÿ≠Ÿäÿßÿ°', 'module': 'ÿßŸÑÿ™ŸÜŸÅÿ≥'}\n",
      "ÿßŸÑÿ™ŸÜŸÅÿ≥\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /quiz HTTP/1.1\" 200 OK\n",
      "{'module': 'ÿßŸÑÿ™ŸÜŸÅÿ≥', 'num_mc': 6, 'num_tf': 4}\n",
      "ÿßŸÑÿ™ŸÜŸÅÿ≥\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:39:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:39:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /quiz HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"OPTIONS /qa HTTP/1.1\" 200 OK\n",
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: chapter_retriever\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:40:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:40:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:40:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /qa HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"OPTIONS /finish HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:41:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= gemini-2.0-flash; provider = gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ [report] Entering report_endpoint\n",
      "['ŸÖŸÑÿÆŸëÿµ ÿßŸÑÿØÿ±ÿ≥:\\n[{\"title\": \"ÿØÿ±ÿ≥ ÿπŸÜ ÿßŸÑÿ™ŸÜŸÇŸÑ\", \"slides\": [{\"number\": \"1\", \"text\": \"ŸÅŸä ÿØÿ±ÿ≥ŸÜÿß ÿßŸÑŸäŸàŸÖÿå ÿ®ÿßÿ¥ ŸÜÿ¥ŸàŸÅŸà ŸÉŸäŸÅÿßÿ¥ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ™ÿ™ŸÜŸÇŸÑ Ÿàÿ™ÿ™ÿ≠ÿ±ŸÉ. ŸÉŸÑ ÿ≠ŸäŸàÿßŸÜ ÿπŸÜÿØŸà ÿ∑ÿ±ŸäŸÇÿ© ÿÆÿßÿµÿ© ÿ®ŸäŸá ÿ®ÿßÿ¥ ŸäŸÖÿ¥Ÿäÿå Ÿäÿ∑Ÿäÿ±ÿå ŸäÿπŸàŸÖ... ŸÉŸäŸÖÿß ÿ£ŸÜÿ™ ÿπŸÜÿØŸÉ ÿ≥ÿßŸÇŸäŸÜ ÿ™ŸÖÿ¥Ÿä ÿ®ŸäŸáŸÖ!\", \"image\": \"assets/book_images/page_27_img_28.jpeg\"}, {\"number\": \"2\", \"text\": \"<b>ÿ£ŸÜŸÖÿßÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ ÿπŸÜÿØ ÿßŸÑÿ≠ŸäŸàÿßŸÜ:</b> ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ™ÿ™ŸÜŸÇŸÑ ÿ®ÿ∑ÿ±ŸÇ ŸÖÿÆÿ™ŸÑŸÅÿ©: ŸÅŸÖÿß ÿßŸÑŸÑŸä ŸäŸÖÿ¥Ÿä ÿπŸÑŸâ ÿ≥ÿßŸÇŸäŸá ŸÉŸäŸÖÿß ÿßŸÑŸÉŸÑÿ®ÿå ŸàÿßŸÑŸÑŸä Ÿäÿ∑Ÿäÿ± ÿ®ÿ¨ŸÜÿßÿ≠ÿßÿ™Ÿá ŸÉŸäŸÖÿß ÿßŸÑÿπÿµŸÅŸàÿ±ÿå ŸàÿßŸÑŸÑŸä ŸäÿπŸàŸÖ ÿ®ÿ≤ÿπÿßŸÜŸÅŸá ŸÉŸäŸÖÿß ÿßŸÑÿ≠Ÿàÿ™. ŸÉŸÑ ÿ≠ŸäŸàÿßŸÜ ŸàÿπŸÜÿØŸà ÿ∑ÿ±ŸäŸÇÿ™Ÿà!\", \"image\": \"assets/book_images/page_26_img_24.jpeg\"}, {\"number\": \"3\", \"text\": \"ŸÖÿ´ÿßŸÑ: ÿßŸÑÿ≠ŸÖÿßŸÖÿ© ÿ™ÿ∑Ÿäÿ± ŸÅŸä ÿßŸÑÿ¨Ÿàÿå ŸàÿßŸÑÿ®ÿ∑ÿ© ÿ™ÿπŸàŸÖ ŸÅŸä ÿßŸÑŸÖÿßÿ°. ÿßŸÑŸÜŸÖÿ± Ÿäÿ¨ÿ±Ÿä ŸÅŸä ÿßŸÑÿ∫ÿßÿ®ÿ©ÿå ŸàÿßŸÑÿ≥ŸÖŸÉÿ© ÿ™ÿ≥ÿ®ÿ≠ ŸÅŸä ÿßŸÑÿ®ÿ≠ÿ±. ŸÑÿßÿ≠ÿ∏ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿßŸÑŸÑŸä ÿ™ÿ±ÿßŸáÿß ŸÅŸä ÿ≠ŸàŸÖÿ™ŸÉÿå ŸÉŸäŸÅÿßÿ¥ ÿ™ÿ™ÿ≠ÿ±ŸÉÿü\", \"image\": \"assets/book_images/page_26_img_23.jpeg\"}, {\"number\": \"4\", \"text\": \"<b>ÿ™ŸÉŸäŸÅ ÿßŸÑÿπÿ∂Ÿà ŸÖÿπ ŸÜŸÖÿ∑ ÿßŸÑÿ™ŸÜŸÇŸÑ:</b> ÿ±ÿ®Ÿä ÿ≥ÿ®ÿ≠ÿßŸÜŸá ÿπÿ∑Ÿâ ŸÑŸÉŸÑ ÿ≠ŸäŸàÿßŸÜ ÿ£ÿπÿ∂ÿßÿ° ÿ™ÿ≥ÿßÿπÿØŸà ÿπŸÑŸâ ÿßŸÑÿ™ŸÜŸÇŸÑ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿπŸäŸÜÿ©. ŸÖÿ´ŸÑÿßŸãÿå ÿßŸÑÿ£ÿ±ŸÜÿ® ÿπŸÜÿØŸà ÿ≥ÿßŸÇŸäŸÜ ŸÇŸàŸäŸäŸÜ ÿ®ÿßÿ¥ ŸäŸÇŸÅÿ≤ÿå ŸàÿßŸÑÿ≥ŸÖŸÉÿ© ÿπŸÜÿØŸáÿß ÿ≤ÿπÿßŸÜŸÅ ÿ®ÿßÿ¥ ÿ™ÿπŸàŸÖ.\", \"image\": \"assets/book_images/page_31_img_38.jpeg\"}, {\"number\": \"5\", \"text\": \"ŸÖÿ´ÿßŸÑ: ÿ≥ÿßŸÇŸäŸÜ ÿßŸÑÿ≠ÿµÿßŸÜ ŸÇŸàŸäŸäŸÜ ÿ®ÿßÿ¥ Ÿäÿ¨ÿ±Ÿä ÿ®ÿ≥ÿ±ÿπÿ©ÿå Ÿàÿ¨ŸÜÿßÿ≠ÿßÿ™ ÿßŸÑŸÜÿ≥ÿ± ŸÉÿ®ÿßÿ± ÿ®ÿßÿ¥ Ÿäÿ∑Ÿäÿ± ŸÅŸä ÿßŸÑÿ≥ŸÖÿßÿ°. ŸÑÿßÿ≠ÿ∏ ŸÉŸäŸÅÿßÿ¥ ÿ¥ŸÉŸÑ ÿ¨ÿ≥ŸÖ ÿßŸÑÿ≠ŸäŸàÿßŸÜ Ÿäÿ≥ÿßÿπÿØŸà ÿπŸÑŸâ ÿßŸÑÿ≠ÿ±ŸÉÿ©.\", \"image\": \"assets/book_images/page_31_img_37.jpeg\"}, {\"number\": \"6\", \"text\": \"ŸÅŸä ÿßŸÑŸÜŸáÿßŸäÿ©ÿå ÿ™ÿπŸÑŸÖŸÜÿß ÿ£ŸÜŸà ŸÉŸÑ ÿ≠ŸäŸàÿßŸÜ ÿπŸÜÿØŸà ÿ∑ÿ±ŸäŸÇÿ© ÿÆÿßÿµÿ© ÿ®ŸäŸá ÿ®ÿßÿ¥ Ÿäÿ™ŸÜŸÇŸÑÿå ŸàÿßŸÑÿ£ÿπÿ∂ÿßÿ° ŸÖÿ™ÿßÿπŸà ŸÖÿ™ŸÉŸäŸÅÿ© ŸÖÿπ ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ™ŸÜŸÇŸÑ Ÿáÿ∞ŸäŸÉÿß. ÿ•ÿ∞ÿß ÿπŸÜÿØŸÉ ÿ£Ÿä ÿ≥ÿ§ÿßŸÑÿå ÿ£ŸÜÿß ŸÖŸàÿ¨ŸàÿØ!\"}]}]', 'ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© Ÿà ÿßŸÑÿ£ÿ¨Ÿàÿ®ÿ©:\\n‚ùì ŸÅÿ≥ÿ±ŸÑŸä ÿßŸÜŸàÿßÿπ ÿßŸÑÿ™ŸÜŸÇŸÑ\\nüì• ÿ®ÿßÿ¥ ŸÜŸÅÿ≥Ÿëÿ±ŸÑŸÉ ÿ£ŸÜŸàÿßÿπ ÿßŸÑÿ™ŸÜŸÇŸÑ Ÿäÿß ŸàŸÑŸäÿØŸäÿå ŸäŸÑÿ≤ŸÖ ŸÜŸÅŸáŸÖŸà ÿ£ŸàŸÑ ÿ≠ÿßÿ¨ÿ© ÿ¥ŸÜŸàÿ© ŸÖÿπŸÜÿßŸáÿß \"ÿ™ŸÜŸÇŸÑ\". ÿßŸÑÿ™ŸÜŸÇŸÑ ŸáŸà ŸÉŸäŸÅÿßÿ¥ ÿßŸÑŸÉÿßÿ¶ŸÜ ÿßŸÑÿ≠Ÿä Ÿäÿ™ÿ≠ÿ±ŸÉ ŸÖŸÜ ÿ®ŸÑÿßÿµÿ© ŸÑÿ®ŸÑÿßÿµÿ©. ÿ™ÿÆŸäŸÑ ÿ±Ÿàÿ≠ŸÉ ÿ™ŸÑÿπÿ® ÿ®ÿßŸÑŸÉŸàÿ±ÿ©ÿå ÿ™ÿ¨ÿ±Ÿä ÿ®ÿßÿ¥ ÿ™ŸàÿµŸÑŸáÿßÿå Ÿáÿ∞ÿßŸÉÿß ŸáŸà ÿßŸÑÿ™ŸÜŸÇŸÑ!\\n\\nÿπŸÜÿØŸÜÿß ÿ£ŸÜŸàÿßÿπ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÖÿ™ÿßÿπ ÿ™ŸÜŸÇŸÑ ÿπŸÜÿØ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ÿå ŸÉŸÑ ÿ≠ŸäŸàÿßŸÜ ŸàŸÉŸäŸÅÿßÿ¥ Ÿäÿ™ÿ≠ÿ±ŸÉ:\\n\\n*   **ÿßŸÑŸÖÿ¥Ÿä ŸàÿßŸÑÿ¨ÿ±Ÿä:** ŸÉŸäŸÖÿß ÿ£ŸÜÿ™ ÿ™ŸÖÿ¥Ÿä Ÿàÿ™ÿ¨ÿ±Ÿäÿå ŸÅŸÖÿß ÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ£ÿÆÿ±Ÿâ ÿ™ÿπŸÖŸÑ ŸÉŸäŸÅŸÉÿå ŸÉŸäŸÖÿß ÿßŸÑŸÉŸÑÿ® ŸàÿßŸÑŸÇÿ∑Ÿàÿ≥ ŸàÿßŸÑÿ≠ÿµÿßŸÜ. Ÿäÿ≥ÿ™ÿπŸÖŸÑŸàÿß ÿ≥ÿßŸÇŸäŸáŸÖ ÿ®ÿßÿ¥ ŸäŸÖÿ¥ŸäŸà ŸàŸäÿ¨ÿ±Ÿà.\\n*   **ÿßŸÑŸÇŸÅÿ≤:** ŸÉŸäŸÖÿß ÿßŸÑŸÉŸÜÿ∫ÿ± ŸàÿßŸÑÿ£ÿ±ŸÜÿ®ÿå Ÿäÿ≥ÿ™ÿπŸÖŸÑŸàÿß ÿ≥ÿßŸÇŸäŸáŸÖ ÿßŸÑÿÆŸÑŸÅŸäÿ© ÿßŸÑŸÇŸàŸäÿ© ÿ®ÿßÿ¥ ŸäŸÇŸÅÿ≤Ÿàÿß ŸàŸäŸÜÿ∑Ÿàÿß. ÿ™ÿÆŸäŸÑ ÿ±Ÿàÿ≠ŸÉ ÿ™ŸÑÿπÿ® \"ÿßŸÑÿ≠ÿ¨ŸÑÿ©\"ÿå Ÿáÿ∞ÿßŸÉÿß ŸáŸà ÿßŸÑŸÇŸÅÿ≤!\\n*   **ÿßŸÑÿ≤ÿ≠ŸÅ:** ŸÉŸäŸÖÿß ÿßŸÑÿ´ÿπÿ®ÿßŸÜÿå ŸÖÿßÿπŸÜÿØŸàÿ¥ ÿ≥ÿßŸÇŸäŸÜÿå Ÿäÿ≤ÿ≠ŸÅ ÿπŸÑŸâ ÿ®ÿ∑ŸÜŸá ÿ®ÿßÿ¥ Ÿäÿ™ÿ≠ÿ±ŸÉ.\\n*   **ÿßŸÑÿ≥ÿ®ÿßÿ≠ÿ©:** ŸÉŸäŸÖÿß ÿßŸÑÿ≠Ÿàÿ™ ŸàÿßŸÑÿ®ÿ∑ÿ©ÿå Ÿäÿ≥ÿ™ÿπŸÖŸÑŸàÿß ÿ≤ÿπÿßŸÜŸÅŸáŸÖ Ÿàÿ£ÿ±ÿ¨ŸÑŸáŸÖ ÿ®ÿßÿ¥ ŸäÿπŸàŸÖŸàÿß ŸÅŸä ÿßŸÑŸÖÿßÿ°.\\n*   **ÿßŸÑÿ∑Ÿäÿ±ÿßŸÜ:** ŸÉŸäŸÖÿß ÿßŸÑÿ∑ŸäŸàÿ± ŸàÿßŸÑŸÅÿ±ÿßÿ¥ÿßÿ™ÿå Ÿäÿ≥ÿ™ÿπŸÖŸÑŸàÿß ÿ£ÿ¨ŸÜÿ≠ÿ™ŸáŸÖ ÿ®ÿßÿ¥ Ÿäÿ∑Ÿäÿ±Ÿàÿß ŸÅŸä ÿßŸÑÿ≥ŸÖÿßÿ°.\\n\\nŸÉŸÑ ÿ≠ŸäŸàÿßŸÜ ÿπŸÜÿØŸá ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ™ŸÜŸÇŸÑ ÿßŸÑÿÆÿßÿµÿ© ÿ®ŸäŸá ÿßŸÑŸÑŸä ÿ™ÿ≥ÿßÿπÿØŸá ÿ®ÿßÿ¥ ŸäÿπŸäÿ¥ ŸàŸäÿßŸÉŸÑ ŸàŸäŸáÿ±ÿ® ŸÖŸÜ ÿßŸÑÿÆÿ∑ÿ±. ŸÅŸáŸÖÿ™ Ÿäÿß ŸàŸÑŸäÿØŸäÿü']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n",
      "\u001b[92m13:41:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     197.26.235.238:0 - \"POST /finish HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"GET /reports/session_report.pdf HTTP/1.1\" 200 OK\n",
      "INFO:     197.26.235.238:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [890]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Learning Assistant (Arabic/Tunisian) ‚Äì CLI + PDF + FastAPI + Quiz\n",
    "Refactored for efficiency: single retriever/LLM/agents initialization\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Learning Assistant (Arabic/Tunisian) ‚Äì CLI + PDF + FastAPI + Quiz\n",
    "Refactored for efficiency: single retriever/LLM/agents initialization\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, re, os, math\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple, Type  # Added Type import\n",
    "\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "\n",
    "from crewai import Agent, Crew, Task, LLM\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "import nest_asyncio, uvicorn\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Paths & Constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "PDF_PATH  = Path(\"/kaggle/input/i9adh-3elmi-sana-4/-     .pdf\")\n",
    "IMG_DIR   = Path(\"/kaggle/input/book-images\")\n",
    "SAVE_PATH = Path(\"/kaggle/working/lessons\")\n",
    "SAVE_PATH.mkdir(exist_ok=True, parents=True)\n",
    "emb    = HuggingFaceEmbeddings(model_name=\"Omartificial-Intelligence-Space/GATE-AraBert-v1\")\n",
    "cross  = HuggingFaceCrossEncoder(model_name=\"Omartificial-Intelligence-Space/ARA-Reranker-V1\")\n",
    "ARABIC_FONT_PATH = \"/kaggle/input/nottoooo/NotoNaskhArabic-Regular.ttf\"     # ‚Üê change if needed\n",
    "ARABIC_FONT_NAME = \"NotoArabic\"\n",
    "ARABIC_FONT_PATH_bold = \"/kaggle/input/text-bold/NotoNaskhArabic-Bold.ttf\"     # ‚Üê change if needed\n",
    "ARABIC_FONT_NAME_bold = \"NotoArabic-Bold\"\n",
    "\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Utilities ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class SessionMemory(dict):\n",
    "    def log(self, key: str, value: Any):\n",
    "        self.setdefault(key, []).append(value)\n",
    "\n",
    "\n",
    "def rtl(txt: str) -> str:\n",
    "    return get_display(arabic_reshaper.reshape(txt))\n",
    "\n",
    "\n",
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    dot = sum(x*y for x,y in zip(a,b))\n",
    "    n1  = math.sqrt(sum(x*x for x in a))\n",
    "    n2  = math.sqrt(sum(y*y for y in b))\n",
    "    return dot/(n1*n2) if n1 and n2 else 0.0\n",
    "\n",
    "def _clean_user_question(raw: str) -> str:\n",
    "    l = raw.strip().lower()\n",
    "    return raw.split(':',1)[1].strip() if l.startswith(('ÿ≥ÿ§ÿßŸÑ:','qa:')) else raw.strip()\n",
    "\n",
    "\n",
    "def _clean_json(text: str) -> str:\n",
    "    t = re.sub(r'```[a-zA-Z]*\\n?','', text).strip()\n",
    "    return t.strip('`').strip()\n",
    "\n",
    "\n",
    "def parse_quiz_json(raw: str) -> Any:\n",
    "    cleaned = _clean_json(raw).replace(\"'\", '\"')\n",
    "    return json.loads(cleaned)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Retriever & LLM Setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def build_retriever(\n",
    "    pdf_path: Path,\n",
    "    emb=emb,\n",
    "    cross=cross,\n",
    "    k_fetch: int = 8,\n",
    "    k_rerank: int = 3\n",
    ") -> ContextualCompressionRetriever:\n",
    "    docs   = load_arabic_pdf(pdf_path)\n",
    "    splits = SemanticChunker(emb).split_documents(docs)\n",
    "    vect   = Chroma.from_documents(splits, emb)\n",
    "    base   = vect.as_retriever(search_kwargs={\"k\": k_fetch})\n",
    "    comp = CrossEncoderReranker(model=cross, top_n=k_rerank)\n",
    "    return ContextualCompressionRetriever(base_retriever=base, base_compressor=comp)\n",
    "\n",
    "# Initialize once\n",
    "global_mem = SessionMemory()  # <== added\n",
    "RETRIEVER = build_retriever(PDF_PATH,emb,cross)\n",
    "LLM_MODEL = LLM(model=\"gemini/gemini-2.0-flash\", temperature=0.5, max_tokens=\"4000\")\n",
    "QA_MEMORY = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ChapterRetriever Tool ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class ChapterRetrieverInput(BaseModel):\n",
    "    query: str = Field(..., description=\"ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ£Ÿà ÿπŸÜŸàÿßŸÜ ÿßŸÑÿØÿ±ÿ≥\")\n",
    "\n",
    "class ChapterRetrieverTool(BaseTool):\n",
    "    name: str = \"chapter_retriever\"  # Annotated with type\n",
    "    description: str = \"Ÿäÿ¨Ÿäÿ® ŸÖŸÇÿßÿ∑ÿπ ŸÖŸÜ ÿßŸÑŸÉÿ™ÿßÿ® ÿ≠ÿ≥ÿ® ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ£Ÿà ÿπŸÜŸàÿßŸÜ ÿßŸÑÿØÿ±ÿ≥.\"  # Annotated with type\n",
    "    args_schema: Type[BaseModel] = ChapterRetrieverInput  # Annotated with Type\n",
    "\n",
    "    def __init__(self, retriever: ContextualCompressionRetriever):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, '_retriever', retriever)\n",
    "\n",
    "    def _run(self, query: str, **kwargs) -> List[str]:\n",
    "        return [d.page_content for d in self._retriever.invoke(query)]\n",
    "\n",
    "# Rebuild Pydantic model to finalize annotations\n",
    "ChapterRetrieverTool.model_rebuild()\n",
    "\n",
    "# Instantiate tool and agents once\n",
    "tool = ChapterRetrieverTool(RETRIEVER)\n",
    "\n",
    "SUMMARY_AGENT = Agent(\n",
    "    role=\"ŸÖŸÑÿÆŸëÿµ ÿßŸÑÿØÿ±ÿ≥\",\n",
    "    goal=\"ŸÖŸÑÿÆŸëÿµ ÿ≥ÿßŸáŸÑ ÿ®ÿßŸÑÿØÿßÿ±ÿ¨ÿ©\",\n",
    "    backstory=\"ŸÖÿπŸÑŸëŸÖÿ© ÿ™Ÿàÿ∂Ÿëÿ≠ ÿßŸÑÿØÿ±Ÿàÿ≥.\",\n",
    "    llm=LLM_MODEL,\n",
    "    tools=[tool],\n",
    "    verbose=False\n",
    ")\n",
    "QA_AGENT = Agent(\n",
    "    role=\"ŸÖÿπŸÑŸëŸÖ Ÿäÿ¨ÿßŸàÿ® ÿπŸÑŸâ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ©\",\n",
    "    goal=\"ŸäŸÇÿØŸÖ ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿØŸÇŸäŸÇÿ© ŸàŸÖÿ®ÿ≥Ÿëÿ∑ÿ© ÿπŸÑŸâ ÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ™ŸÑŸÖŸäÿ∞.\",\n",
    "    backstory=\"ŸÖÿπŸÑŸëŸÖ ÿ™ŸàŸÜÿ≥Ÿä ÿµÿ®Ÿàÿ±.\",\n",
    "    llm=LLM_MODEL,\n",
    "    tools=[tool],\n",
    "    memory=QA_MEMORY,\n",
    "    verbose=False\n",
    ")\n",
    "QUIZ_AGENT = Agent(\n",
    "    role=\"ÿµÿßŸÜÿπ ÿßŸÑÿßŸÖÿ™ÿ≠ÿßŸÜÿßÿ™\",\n",
    "    goal=\"ŸäÿπŸÖŸÑ ÿ£ÿ≥ÿ¶ŸÑÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© ŸàŸäÿµÿ≠Ÿëÿ≠Ÿáÿß ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿ≠Ÿàÿ± ÿßŸÑŸÖÿ≠ÿØŸëÿØ\",\n",
    "    backstory=\"Ÿäÿ≠ÿ® ÿßŸÑŸÜÿ¨ŸàŸÖ ÿßŸÑÿ∞Ÿáÿ®Ÿäÿ©.\",\n",
    "    llm=LLM_MODEL,\n",
    "    tools=[tool],\n",
    "    verbose=False\n",
    ")\n",
    "FEEDBACK_AGENT = Agent(\n",
    "    role=\"ŸÖÿπÿØŸë ÿßŸÑÿ™ŸÇÿ±Ÿäÿ±\",\n",
    "    goal=\"ŸäŸÉÿ™ÿ® ÿ™ŸÇÿ±Ÿäÿ± PDF ŸÖÿ¥ÿ¨Ÿëÿπ\",\n",
    "    backstory=\"ÿ£ÿÆÿµŸëÿßÿ¶Ÿä ŸÖÿ™ÿßÿ®ÿπÿ© ÿ™ÿπŸÑŸÖ.\",\n",
    "    llm=LLM_MODEL,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Helper Functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def retrieve_context(topic: str, kg) -> Tuple[str, str]:\n",
    "    lessons = kg.get_lessons_for_topic(topic)\n",
    "    text_chunks = []\n",
    "    images_blocks = []\n",
    "    for ld in lessons:\n",
    "        text_chunks.extend(tool.run(ld['title']))\n",
    "        pics = kg.fetch_lesson_images(ld['title'])\n",
    "        if pics:\n",
    "            md = \"\\n\".join(f\"* [{i['caption']}]({i['name']})\" for i in pics)\n",
    "            images_blocks.append(f\"ÿØÿ±ÿ≥ ¬´{ld['title']}¬ª ‚Äì ÿßŸÑÿ™ÿµÿßŸàÿ±:\\n{md}\\n\")\n",
    "    return \"\\n\".join(text_chunks[:30]), (\"\\n\".join(images_blocks) or \"ŸÖÿß ÿ´ŸÄŸÖŸëŸÄÿ© ÿ≠ÿ™Ÿâ ÿ™ÿµÿßŸàÿ±.\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Summary Generator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_summary_json(user_in: str, kg) -> dict:\n",
    "    m = re.match(r\"ŸÖŸÑÿÆÿµ\\s+(?:ŸÖÿ≠Ÿàÿ±\\s+)?(?P<topic>[\\u0600-\\u06FF ]+)\", user_in)\n",
    "    if not m:\n",
    "        raise ValueError(\"‚ö†Ô∏è ŸÑÿßÿ≤ŸÖ ÿ™ÿ∞ŸÉÿ± ÿßÿ≥ŸÖ ÿßŸÑŸÖÿ≠Ÿàÿ± ÿ®ÿπÿØ ŸÉŸÑŸÖÿ© ¬´ŸÖŸÑÿÆÿµ¬ª.\")\n",
    "    topic = m.group('topic').strip()\n",
    "    branch = kg.find_branch_for_topic(topic)\n",
    "    lessons_info = kg.get_lessons_for_topic(topic)\n",
    "    if not branch or not lessons_info:\n",
    "        raise LookupError(f\"‚ö†Ô∏è ŸÖÿß ŸÑŸÇŸäÿ™ÿ¥ ÿßŸÑŸÖÿ≠Ÿàÿ± ¬´{topic}¬ª ŸÅŸä ÿßŸÑŸÄ KG.\")\n",
    "\n",
    "    ctx_text, images_section = retrieve_context(topic, kg)\n",
    "    sub_lessons_md = \"\\n\".join(f\"‚Ä¢ {ld['title']}\" for ld in lessons_info)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ÿ•ŸÜÿ™Ÿä ŸÖÿπŸÑŸëŸÖ/ÿ© ÿ™ŸàŸÜÿ≥Ÿä/ÿ©ÿõ ŸáÿØŸÅŸÉ ÿ™ÿ®ÿ≥Ÿëÿ∑ ŸÖÿ≠Ÿàÿ± ‚Äú{topic}‚Äù ŸÖŸÜ ŸÅÿ±ÿπ ‚Äú{branch}‚Äù ŸÑÿ™ŸÑŸÖŸäÿ∞ ŸÅŸä\n",
    "ÿßŸÑÿ≥ŸÜÿ© ÿßŸÑÿ±ÿßÿ®ÿπÿ© ÿßÿ®ÿ™ŸÄÿØÿßÿ¶Ÿä. ÿ±ŸÉŸëÿ≤ ÿπŸÑŸâ ÿßŸÑŸÅŸáŸÖÿå ÿ±ÿ®ÿ∑ ÿßŸÑÿ£ŸÅŸÉÿßÿ± ÿ®ÿ≠Ÿäÿßÿ™Ÿà ÿßŸÑŸäŸàŸÖŸäÿ©ÿå\n",
    "Ÿàÿ™ŸÜŸàŸäÿπ ÿßŸÑÿ£ŸÖÿ´ŸÑÿ©.\n",
    "\n",
    "ÿßŸÑŸÖÿπÿ∑Ÿäÿßÿ™ ŸÇÿØÿßŸÖŸÉ:\n",
    "‚îå‚îÄ ÿßŸÑÿØÿ±Ÿàÿ≥ ÿßŸÑŸÅÿ±ÿπŸäŸëÿ©:\n",
    "{sub_lessons_md}\n",
    "\n",
    "‚îå‚îÄ ŸÖŸÇÿ™ÿ∑ŸÅÿßÿ™ ŸÖŸÜ ÿßŸÑŸÉÿ™ÿßÿ® (ÿ™ÿ≥ÿ™ÿπŸÖŸÑŸáÿß ŸÉÿßŸÜ ÿ™ÿ≠ÿ® ÿ™ŸÇÿ™ÿ®ÿ≥ ÿ¨ŸÖŸÑÿ© ŸàŸÑÿß ÿ™Ÿàÿ∂Ÿäÿ≠):\n",
    "{ctx_text}\n",
    "\n",
    "‚îå‚îÄ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ™ÿµÿßŸàÿ± ŸÖÿ±ÿ™ÿ®ÿ∑ÿ© (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä ÿ™ÿ≥ÿ™ÿπŸÖŸÑ ÿ®ÿπÿ∂Ÿáÿß):\n",
    "{images_section}\n",
    "\n",
    "ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿπŸÖŸÑ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©:\n",
    "1) ÿ•ŸÅÿ™ÿ™ÿßÿ≠ŸäŸëÿ© ÿµÿ∫Ÿäÿ±ÿ© ÿ®ÿßŸÑÿØÿßÿ±ÿ¨ÿ© (ÿ≥ÿ∑ÿ±ŸäŸÜ ÿ•ŸÑŸâ Ÿ£ ÿ≥ÿ∑Ÿàÿ±) ÿ™ÿπÿ±ŸëŸÅ ŸÅŸäŸáÿß ÿ®ÿßŸÑŸÖÿ≠Ÿàÿ±\n",
    "   ŸàŸÑŸÖÿßÿ∞ÿß ŸäŸáŸÖŸë ÿßŸÑÿ™ŸÑŸÖŸäÿ∞ ŸÅŸä ÿ≠Ÿäÿßÿ™Ÿà.\n",
    "2)  ÿ®ÿπÿØ ÿßŸÑÿ•ŸÅÿ™ÿ™ÿßÿ≠Ÿäÿ©ÿå ÿßÿπŸÖŸÑ ŸÑŸÉŸÑ ÿØÿ±ÿ≥ ŸÅÿ±ÿπŸä ÿßŸÑŸä ÿπŸÜÿØŸÉ :\n",
    "    ‚Ä¢ ÿßÿ¥ÿ±ÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ÿ®ÿπÿ®ÿßÿ±ÿ© ŸÖÿ®ÿ≥Ÿëÿ∑ÿ©.\n",
    "    ‚Ä¢ ÿ£ÿπÿ∑ ŸÖÿ´ÿßŸÑŸãÿß ŸàÿßŸÇÿπŸäŸãÿß ŸÖŸÜ ÿ≠Ÿäÿßÿ© ÿßŸÑÿ∑ŸÅŸÑ (ÿßŸÑÿØÿßÿ±ÿå ÿßŸÑÿ≠ŸàŸÖÿ©ÿå ÿßŸÑÿ∑ÿ®Ÿäÿπÿ©‚Ä¶).\n",
    "    ‚Ä¢ ÿ•ÿ∞ÿß ÿπŸÜÿØŸÉ ÿµŸàÿ±ÿ© ÿ™Ÿàÿ∂Ÿëÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ©ÿå ÿ£ÿØÿ±ÿ¨Ÿáÿß ŸÅŸä ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿ¥ÿ±ÿßÿ¶ÿ≠ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ¥ŸÉŸÑ:\n",
    "      ![ŸàÿµŸÅ ŸÖÿÆÿ™ÿµÿ±](assets/book_images/page_6_img_0.jpeg)\n",
    "        ‚Ü≥ ÿßŸÑÿµŸàÿ± ÿßÿÆÿ™Ÿäÿßÿ±Ÿäÿ©ÿå ŸàŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ Ÿ£ ÿµŸàÿ± ŸÅŸä ŸÉÿßŸÖŸÑ ÿßŸÑŸÖŸÑÿÆÿµ.\n",
    "3) ÿ£ÿÆÿ™ŸÖ ÿ®ÿ≥ÿ∑ÿ± ŸäŸèŸÑÿÆŸëÿµ ÿßŸÑÿØÿ±ÿ≥ ŸàŸÇŸÑŸàÿß ÿßÿ∞ÿß ÿπŸÜÿØŸÉ ÿ≥ÿ§ÿßŸÑ ÿßŸÜŸÉ ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿÆÿßŸÜÿ© ÿßÿ≥ÿ¶ŸÑŸÜŸä.\n",
    "\n",
    "ÿ™ŸÜÿ®ŸäŸáÿßÿ™ ÿ£ÿ≥ŸÑŸàÿ®ŸäŸëÿ©:\n",
    "- ÿßŸÉÿ™ÿ® ÿ®ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ© ÿßŸÑÿÆŸÅŸäŸÅÿ©ÿå ÿ¨ŸèŸÖŸéŸÑ ŸÇÿµŸäÿ±ÿ©ÿå ŸÖŸÅÿ±ÿØÿßÿ™ ŸÖÿ£ŸÑŸàŸÅÿ©.\n",
    "- ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ£ŸÅÿπÿßŸÑ ÿ£ŸÖÿ± ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©: ¬´ÿ¨ÿ±Ÿëÿ®¬ªÿå ¬´ÿ±ŸÉŸëÿ≤¬ªÿå ¬´ŸÑÿßÿ≠ÿ∏¬ª.\n",
    "- ŸÑÿß ÿ™ÿ∞ŸÉÿ± ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑÿµŸÅÿ≠ÿßÿ™ ŸàŸÑÿß ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿØÿßÿÆŸÑ ÿßŸÑŸÜÿµ (ÿ•ŸÑÿß ŸÅŸä ÿµŸäÿ∫ÿ© ÿßŸÑŸÖÿßÿ±ŸÉÿØÿßŸàŸÜ ![alt](path)).\n",
    "\n",
    "‚¨áÔ∏è **ÿßŸÑŸÖÿÆÿ±ÿ¨ÿßÿ™ Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ŸÉŸàŸÜ JSON ŸÅŸÇÿ∑ÿå ŸÖÿ∑ÿßÿ®ŸÇŸãÿß ŸÑŸáÿ∞ÿß ÿßŸÑŸáŸäŸÉŸÑ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑** ‚¨áÔ∏è\n",
    "{{\n",
    "  \"title\": \"ÿØÿ±ÿ≥ ÿπŸÜ {topic}\",\n",
    "  \"slides\": [\n",
    "    {{ \"number\": \"1\", \"text\": \"ÿ¥ÿ±ÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ£ŸàŸÑŸâ ŸáŸÜÿß\", \"image\": \"assets/book_images/page_6_img_0.jpeg\" }},\n",
    "    {{ \"number\": \"2\", \"text\": \"ÿ¥ÿ±ÿ≠ ÿßŸÑŸÅŸÉÿ±ÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ© ŸáŸÜÿß\"}},\n",
    "    {{ \"number\": \"3\", \"text\": \"‚Ä¶\", \"image\": \"assets/book_images/page_6_img_2.jpeg\" }},\n",
    "    //  ÿ£ÿ∂ŸÅ ÿ¥ÿ±ÿßÿ¶ÿ≠ ÿ£ÿÆÿ±Ÿâ ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ®ŸÜŸäÿ©ÿõ ÿ•ÿ∞ÿß ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿµŸàÿ±ÿ©ÿå ŸÑÿß ÿ™ÿ∂ŸÅ ÿ≠ŸÇŸÑ ÿßŸÑÿµŸàÿ±ÿ© ŸÉÿßŸÑŸÖÿ´ÿßŸÑ ÿ±ŸÇŸÖ ÿßÿ´ŸÜÿßŸÜ\n",
    "  ]\n",
    "}}\n",
    "\"\"\" \n",
    "    print(prompt)\n",
    "    task = Task(description=prompt, expected_output=\"json\", agent=SUMMARY_AGENT)\n",
    "    raw = Crew(agents=[SUMMARY_AGENT], tasks=[task], verbose=False).kickoff().raw\n",
    "    cleaned = _clean_json(raw)\n",
    "    start = cleaned.find('{'); end = cleaned.rfind('}')\n",
    "    if start<0 or end<0:\n",
    "        raise HTTPException(502, \"No JSON object found in LLM output\")\n",
    "    data = json.loads(cleaned[start:end+1])\n",
    "\n",
    "    filename = f\"{branch}_{topic}.json\".replace(' ', '_')\n",
    "    path = SAVE_PATH/filename\n",
    "    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    return {\"path\": f\"/lessons/{filename}\", \"data\": data}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ QA Handler ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def handle_qa(question: str, kg,emb) -> str:\n",
    "    # 1) clean question\n",
    "    q = _clean_user_question(question)\n",
    "\n",
    "    q_emb = emb.embed_query(q)\n",
    "    all_lessons = kg.fetch_all_lesson_embeddings()\n",
    "    best_score, topic, lesson = max(\n",
    "        ((cosine_similarity(q_emb, e['embedding']), e['topic'], e['lesson']) for e in all_lessons),\n",
    "        default=(0.0, None, None)\n",
    "    )\n",
    "\n",
    "    # 3) load previous conversation\n",
    "    mem_vars = QA_MEMORY.load_memory_variables({})\n",
    "    history = mem_vars.get(\"chat_history\", \"\")\n",
    "\n",
    "    # 4) build prompt including memory\n",
    "    if best_score >= 0.25 and topic:\n",
    "        ctx_text, _ = retrieve_context(topic, kg)\n",
    "        sub_md = \"\\n\".join(f\"‚Ä¢ {ld['title']}\" for ld in kg.get_lessons_for_topic(topic))\n",
    "        prompt = (\n",
    "            f\"ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©:\\n{history}\\n\\n\"\n",
    "            f\"ÿ£ŸÜÿ™ ŸÖÿπŸÑŸëŸÖ ÿµÿ®Ÿàÿ± ŸàŸÑÿ∑ŸäŸÅ. ÿπŸÜÿØŸÉ Ÿáÿ∞Ÿá ÿßŸÑÿ≥ÿ§ÿßŸÑ ŸÖŸÜ ÿßŸÑÿ∑ŸÅŸÑ:\\n¬´{q}¬ª\\n\\n\"\n",
    "            f\"ÿØÿ±ÿ≥ ‚Äú{lesson}‚Äù ÿ™ÿ≠ÿ™ ŸÖÿ≠Ÿàÿ± ‚Äú{topic}‚Äù ŸáŸà ÿßŸÑÿ£ŸÜÿ≥ÿ®.\\n\"\n",
    "            f\"Ÿáÿ∞Ÿá ŸÇÿßÿ¶ŸÖÿ© ÿßŸÑÿØÿ±Ÿàÿ≥:\\n{sub_md}\\n\\n\"\n",
    "            \"ŸÇÿØŸëŸÖ ÿ¥ÿ±ÿ≠Ÿãÿß ÿ™ŸÅÿµŸäŸÑŸäŸãÿß ÿ®ÿπÿ®ÿßÿ±ÿßÿ™ ÿ®ÿ≥Ÿäÿ∑ÿ©:\\n\"\n",
    "            \"- ÿπÿ±ŸÅ ÿßŸÑŸÖÿµÿ∑ŸÑÿ≠.\\n\"\n",
    "            \"- ŸÖÿ´ÿßŸÑ ŸÖŸÜ ÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑŸäŸàŸÖŸäÿ©.\\n\"\n",
    "            \"- ŸÅÿ≥Ÿëÿ± ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿµÿπŸäÿ®ÿ© ŸÉÿ£ŸÜŸÉ ÿ™ÿ¥ÿ±ÿ≠ ŸÑÿ™ŸÑŸÖŸäÿ∞ ŸÅŸä ÿßŸÑÿ±ÿßÿ®ÿπÿ©.\\n\\n\"\n",
    "            \"ŸÖÿß ÿ™ÿ∞ŸÉÿ±ÿ¥ ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑÿµŸÅÿ≠ÿßÿ™. ÿ£ÿ¨ÿ® ÿ®ŸÑŸáÿ¨ÿ© ÿ™ŸàŸÜÿ≥ŸäŸëÿ©.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©:\\n{history}\\n\\n\"\n",
    "            \"ÿ£ŸÜÿ™ ŸÖÿπŸÑŸëŸÖ ÿµÿ®Ÿàÿ± ŸàŸÑÿ∑ŸäŸÅ. ŸàÿµŸÑÿ™ŸÉ Ÿáÿ∞Ÿá ÿßŸÑÿ≥ÿ§ÿßŸÑ ŸÖŸÜ ÿßŸÑÿ∑ŸÅŸÑ:\\n\"\n",
    "            f\"¬´{q}¬ª\\n\\n\"\n",
    "            \"ŸÇÿØŸëŸÖ ÿ¥ÿ±ÿ≠Ÿãÿß ÿ™ŸÅÿµŸäŸÑŸäŸãÿß ÿ®ÿπÿ®ÿßÿ±ÿßÿ™ ÿ®ÿ≥Ÿäÿ∑ÿ©:\\n\"\n",
    "            \"- ÿπÿ±ŸÅ ÿßŸÑŸÖÿµÿ∑ŸÑÿ≠.\\n\"\n",
    "            \"- ŸÖÿ´ÿßŸÑ ŸÖŸÜ ÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑŸäŸàŸÖŸäÿ©.\\n\"\n",
    "            \"- ŸÅÿ≥Ÿëÿ± ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿµÿπŸäÿ®ÿ© ŸÉÿ£ŸÜŸÉ ÿ™ÿ¥ÿ±ÿ≠ ŸÑÿ™ŸÑŸÖŸäÿ∞ ŸÅŸä ÿßŸÑÿ±ÿßÿ®ÿπÿ©.\\n\\n\"\n",
    "            \"ŸÖÿß ÿ™ÿ∞ŸÉÿ±ÿ¥ ÿ£ÿ±ŸÇÿßŸÖ ÿßŸÑÿµŸÅÿ≠ÿßÿ™. ÿ£ÿ¨ÿ® ÿ®ŸÑŸáÿ¨ÿ© ÿ™ŸàŸÜÿ≥ŸäŸëÿ©.\"\n",
    "        )\n",
    "\n",
    "    # 5) run the agent\n",
    "    task = Task(description=prompt, expected_output=\"text\", agent=QA_AGENT)\n",
    "    answer = Crew(agents=[QA_AGENT], tasks=[task], verbose=False).kickoff().raw\n",
    "\n",
    "    # 6) save to memory\n",
    "    QA_MEMORY.save_context({\"user_input\": q}, {\"assistant_output\": answer})\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Quiz Generator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_quiz_json(module: str, kg, num_mc: int=6, num_tf: int=4) -> dict:\n",
    "    branch = kg.find_branch_for_topic(module)\n",
    "    lessons_info = kg.get_lessons_for_topic(module)\n",
    "    if not branch or not lessons_info:\n",
    "        raise LookupError(f\"‚ö†Ô∏è ŸÖÿß ŸÑŸÇŸäÿ™ÿ¥ ÿßŸÑŸÖÿ≠Ÿàÿ± ¬´{module}¬ª ŸÅŸä ÿßŸÑŸÄ KG.\")\n",
    "    ctx_text,_ = retrieve_context(module, kg)\n",
    "    sub_list = \"\\n\".join(f\"‚Ä¢ {ld['title']} (pages {ld['start_page']}‚Äì{ld['end_page']})\" for ld in lessons_info)\n",
    "    prompt = (\n",
    "        f\"ÿ£ŸÜÿ™ ÿµÿßŸÜÿπ ÿßŸÖÿ™ÿ≠ÿßŸÜÿßÿ™ ŸÑÿ™ŸÑÿßŸÖŸäÿ∞ ÿßŸÑÿ≥ŸÜŸàÿßÿ™ ÿßŸÑÿßÿ®ÿ™ÿØÿßÿ¶Ÿäÿ©. \"\n",
    "        f\"ÿ£ÿπŸêÿØŸí JSON Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ **{num_mc} ÿ£ÿ≥ÿ¶ŸÑÿ© ÿßÿÆÿ™Ÿäÿßÿ± ŸÖŸÜ ŸÖÿ™ÿπÿØÿØ** Ÿà**{num_tf} ÿ£ÿ≥ÿ¶ŸÑÿ© ÿµÿ≠/ÿÆÿ∑ÿ£** \"\n",
    "        f\"ÿπŸÜ ŸÖÿ≠Ÿàÿ± ¬´{module}¬ª ŸÖŸÜ ŸÅÿ±ÿπ ¬´{branch}¬ª. \"\n",
    "        f\"ÿ™ÿ£ŸÉÿØ ÿ£ŸÜ ÿ™Ÿèÿ∫ÿ∑ŸëŸä ÿ¨ŸÖŸäÿπ ÿßŸÑÿØÿ±Ÿàÿ≥ ÿßŸÑŸÅÿ±ÿπŸäÿ©:\\n{sub_list}\\n\\n\"\n",
    "        f\"ÿ®ÿπÿ∂ ŸÖŸÇÿ™ÿ∑ŸÅÿßÿ™ ŸÖŸÜ ÿßŸÑŸÉÿ™ÿßÿ®:\\n{ctx_text}\\n\\n\"\n",
    "        \"ŸáŸäŸÉŸÑ JSON ÿßŸÑŸÖÿ∑ŸÑŸàÿ®:\\n\"\n",
    "        \"\"\"{\n",
    "  \"questions\": [\n",
    "    {\"type\":\"mc\",\"q\":\"...\", \"options\":[\"...\",\"...\",\"...\",\"...\"], \"a\":\"...\"},\n",
    "    {\"type\":\"tf\",\"q\":\"...\",\"options\":[\"ÿµÿ≠\",\"ÿÆÿ∑ÿ£\"] \"a\":\"ÿµÿ≠\"}\n",
    "  ]\n",
    "}\"\"\"\n",
    "    )\n",
    "    task = Task(description=prompt, expected_output=\"json\", agent=QUIZ_AGENT)\n",
    "    raw = Crew(agents=[QUIZ_AGENT], tasks=[task], verbose=False).kickoff().raw\n",
    "    data = parse_quiz_json(raw)\n",
    "    return {\"module\": module, \"data\": data}\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"], allow_credentials=True\n",
    ")\n",
    "app.mount(\"/lessons\", StaticFiles(directory=str(SAVE_PATH)), name=\"lesson_files\")\n",
    "app.mount(\"/reports\", StaticFiles(directory=\"/kaggle/working\"), name=\"reports\")\n",
    "\n",
    "neo_kg   = Neo4jKG(URI, USER, PASSWORD)\n",
    "\n",
    "@app.post(\"/summary\")\n",
    "async def summary_endpoint(req: Request):\n",
    "    body = await req.json()\n",
    "    mod = body.get(\"module\", \"\").strip()\n",
    "    if not mod:\n",
    "        return JSONResponse({\"error\": \"module is required\"}, status_code=400)\n",
    "\n",
    "    try:\n",
    "        user_in = f\"ŸÖŸÑÿÆÿµ ŸÖÿ≠Ÿàÿ± {mod}\"\n",
    "        result  = generate_summary_json(user_in, neo_kg)\n",
    "        global_mem.log('chapter_summary', result['data'])\n",
    "        return JSONResponse(result)\n",
    "\n",
    "    except LookupError as e:                 # üëà  catches branch / lessons not found\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=404)\n",
    "\n",
    "    except Exception as e:\n",
    "        # log full traceback for yourself\n",
    "        import traceback, sys\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        return JSONResponse({\"error\": \"internal failure\"}, status_code=500)\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.post(\"/qa\")\n",
    "async def qa_endpoint(req: Request):\n",
    "    body     = await req.json()\n",
    "    question = body.get(\"question\", \"\").strip()\n",
    "    if not question:\n",
    "        return JSONResponse({\"error\": \"question is required\"}, status_code=400)\n",
    "\n",
    "    try:\n",
    "        answer = handle_qa(question, neo_kg,emb)\n",
    "        global_mem.log('qa_history', (question, answer))\n",
    "        return JSONResponse(answer)\n",
    "\n",
    "    except LookupError as e:\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=404)\n",
    "    except Exception as e:\n",
    "        return JSONResponse({\"error\": \"internal failure\", \"details\": str(e)}, status_code=500)\n",
    "\n",
    "@app.post(\"/quiz\")\n",
    "async def quiz_endpoint(req: Request):\n",
    "    \"\"\"\n",
    "    Expects JSON: { \"module\": \"ÿßÿ≥ŸÖ ÿßŸÑŸÖÿ≠Ÿàÿ±\", \"num_mc\": 6, \"num_tf\": 4 }\n",
    "    \"\"\"\n",
    "    body    = await req.json()\n",
    "    print(body)\n",
    "    module  = body.get(\"module\", \"\").strip()\n",
    "    print(module)\n",
    "    num_mc  = int(body.get(\"num_mc\", 6))\n",
    "    num_tf  = int(body.get(\"num_tf\", 4))\n",
    "    if not module:\n",
    "        return JSONResponse({\"error\": \"module is required\"}, status_code=400)\n",
    "    try:\n",
    "        result = generate_quiz_json(module, neo_kg, num_mc=num_mc, num_tf=num_tf)\n",
    "        global_mem['quiz_log'] = result['data']['questions']\n",
    "        # Initialize quiz_results as placeholder\n",
    "        global_mem['quiz_results'] = {\"correct\": 0, \"incorrect\": len(result['data']['questions'])}\n",
    "        return JSONResponse(result)\n",
    "    except LookupError as e:\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=404)\n",
    "    except Exception as e:\n",
    "        return JSONResponse({\"error\": \"internal failure\", \"details\": str(e)}, status_code=500)\n",
    "@app.post(\"/finish\")\n",
    "async def finish():\n",
    "    print(\"üü¢ [report] Entering report_endpoint\")\n",
    "\n",
    "    # build feedback prompt from session history\n",
    "    parts = []\n",
    "    if 'chapter_summary' in global_mem:\n",
    "        parts.append(\"ŸÖŸÑÿÆŸëÿµ ÿßŸÑÿØÿ±ÿ≥:\\n\" + json.dumps(global_mem['chapter_summary'], ensure_ascii=False))\n",
    "    if 'qa_history' in global_mem:\n",
    "        qa_lines = [f\"‚ùì {q}\\nüì• {a}\" for q, a in global_mem['qa_history']]\n",
    "        parts.append(\"ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© Ÿà ÿßŸÑÿ£ÿ¨Ÿàÿ®ÿ©:\\n\" + \"\\n\".join(qa_lines))\n",
    "    if 'quiz_log' in global_mem and any('child' in q for q in global_mem['quiz_log']):\n",
    "        quiz_questions = global_mem.get('quiz_log', [])\n",
    "        if isinstance(quiz_questions, dict) and 'questions' in quiz_questions:\n",
    "            quiz_questions = quiz_questions['questions']\n",
    "        quiz_lines = [\n",
    "            f\"{i+1}) {q.get('q')} ‚Äì ÿ•ÿ¨ÿßÿ®ÿ© ÿµÿ≠Ÿäÿ≠ÿ©: {q.get('a')}\"\n",
    "            for i, q in enumerate(quiz_questions)\n",
    "        ]\n",
    "        parts.append(\"ÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±:\\n\" + \"\\n\".join(quiz_lines))\n",
    "\n",
    "    print(parts)\n",
    "    fb_prompt = (\n",
    "                \"ÿ£ŸÜÿ™ ÿ£ÿÆÿµŸëÿßÿ¶Ÿä ŸÖÿ™ÿßÿ®ÿπÿ© ÿ™ÿπŸÑŸëŸÖ ŸÑŸÑÿ£ÿ∑ŸÅÿßŸÑ. \"\n",
    "                \"ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸÜ ÿ¨ŸÑÿ≥ÿ™ŸáŸÖ:\\n\\n\"\n",
    "                + \"\\n---\\n\".join(parts)\n",
    "                + \"\\n\\n\"\n",
    "                \"ÿßŸÉÿ™ÿ® ÿ±ÿ≥ÿßŸÑÿ© ÿ™ÿ¥ÿ¨ŸäÿπŸäÿ© ŸÇÿµŸäÿ±ÿ© ÿ®ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ©ÿå ÿ™ŸÑÿÆŸëÿµ ŸÜÿ¨ÿßÿ≠ÿßÿ™ŸáŸÖ Ÿàÿ™ÿ¥ÿ¨ŸëÿπŸáŸÖ ÿπŸÑŸâ ÿßŸÑÿßÿ≥ÿ™ŸÖÿ±ÿßÿ± ŸÅŸä ÿßŸÑÿØÿ±ÿßÿ≥ÿ©.\"\n",
    "            )\n",
    "\n",
    "    fb_task = Task(\n",
    "        description=fb_prompt,\n",
    "        expected_output=\"ÿ±ÿ≥ÿßŸÑÿ© ÿ™ÿ¥ÿ¨ŸäÿπŸäÿ©\",\n",
    "        agent=FEEDBACK_AGENT,\n",
    "    )\n",
    "    fb_note = Crew(agents=[FEEDBACK_AGENT], tasks=[fb_task], verbose=False).kickoff().raw\n",
    "    global_mem[\"feedback_note\"] = fb_note\n",
    "\n",
    "    pdf_path = Path(\"/kaggle/working/session_report.pdf\")\n",
    "    render_pdf(global_mem, pdf_path)\n",
    "\n",
    "    # Return the direct URL to the static file!\n",
    "    return JSONResponse({\n",
    "        \"pdf_url\": \"/reports/session_report.pdf\"\n",
    "    })\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Initialize & Run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    os.environ['ngrok_authToken']='2yMaZ6btidIIiv3fwpkG287hAOT_2ezDgPqKcpGa2w9Z3WpxT'\n",
    "    conf.get_default().auth_token = os.environ[\"ngrok_authToken\"]\n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(\"Public URL:\", public_url)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7598875,
     "sourceId": 12071821,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7612236,
     "sourceId": 12092243,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7631814,
     "sourceId": 12120485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7634088,
     "sourceId": 12123918,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7634727,
     "sourceId": 12124915,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7639496,
     "sourceId": 12131394,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
